---
title:  "用Solr打造定制的搜索引擎"
layout: post
mathjax: true
tags: java web
---

在数据分析、站内搜索等应用中，我们往往需要有自己的搜索服务。[Apache Solr](http://lucene.apache.org/solr/)（用户包括Instagram、Netflix和Digg）和[ElasticSearch](https://www.elastic.co/cn/products/elasticsearch)（用户包括GitHub、Quora和eBay）就是目前最流行的开源搜索服务器软件，无独有偶，它们都基于用Java语言编写的搜索引擎[Apache Lucene](http://lucene.apache.org)。

## 开始使用Solr

1. 确保Java运行时环境已经安装好
2. [下载Solr](https://lucene.apache.org/solr/mirrors-solr-latest-redir.html)
3. 进入解压后目录
4. 运行`bin/solr start -c`启动Solr云
5. 运行`bin/solr create -c 集合名 -s 2 -rf 2`创建一个集合，其中`-s 2`表示集合索引数据分散到两个节点、而`-rf 2`表示所有索引数据有两个复本
6. 回到命令行运行`bin/post -c 集合名 需要索引的文件列表`以索引一些你想以后能搜索到的文件
7. 在浏览器打开[http://localhost:8983/solr/](http://localhost:8983/solr/)进入控制台
8. 从左下方选择刚创建的集合
9. 在`Query`选项卡中进行查询

## 搜索引擎的工作方式

设想有一本书（如《电工手册》），我们想知道里面有没有一个词（如“无刷电机”），有的话在哪些段落中。如果这本书没有索引，则我们可能需要把整本书看完才知道，除非十分走运，这个词刚好出现在我们一开始翻的哪几页。对于一本书，还可以勉为其难地这样干，但如果要在整个大型图书馆中这样去找一个词出现在哪些书中则可以认为是不可能的任务。如果这本书有索引，我们只用在索引中找有没有“无刷电机”这个词，由于索引中各个词是排好序的，能够很快地定位它，然后有的话索引就告诉我们这个词在哪页出现。如果希望的话，我们甚至可以通过把图书馆中所有书的索引合并起来。由于图书的物理结构是把位置自然地对应的词，这种把词对应到位置的索引也叫倒排索引。我们要强调的是，建立索引是一次性的，却可以为大大缩短了查询所需的时间。

对于数字数据也是同样道理，我们对需要搜索到的数据做一次索引，然后查询就能基于索引而不是原始数据进行，这样查询可能做到非常快。和图书一样，索引本质上是个映射，把关键词对应到它所在文档的列表。倒排索引有不同的实现方式，例如映射表可以是B+树、分布式散列表或者其它数据结构，文档列表可以用链表、位图或者其它数据结构实现，但这些细节还是留给Lucene的开发者去考虑好了。

建立索引时，需要进行以下工作：
1. 导入数据。要索引文档首先要取得文档，我们把文档看作由一些字段组成。导入文档有以下途径：
    - 来自文件
        - 对于XML、JSON、CSV等格式的文件，它们清晰地给出了结构化数据
        - 对于PDF、Microsoft Office、OpenDocument、RTF、HTML、TXT、LOG等格式的文档，还需要一些程序来进行解析并提取出各字段
    - 来自数据库
2. 分析各个字段的值，
    1. 分词。把字段值分解成单词有许多不同方法，如单词满足的正则表达式、分隔符满足的正则表达式、N-Gram。对于欧洲语言，分词相对简单，不论以最长字母序列作为单词还是在空白和标点符号处切分都有不错的效果。但对于中日韩语言，要准确切分并不容易，基于词典的最长前缀切分要求词典不断更新（这也是许多搜索引擎也做输入法收集数据的原因，但每次更新词典后更新索引成本太高，不然又会影响搜索质量），有时干脆把每个字符或N-Gram（如“万里长城永不倒”分成2-Gram“万里”、“里长”、“长城”、“城永”、“永不”、“不倒”）当作单词。
    2. 过滤。对单词序列往往会进行一些变换，比如：
        - 统一大小写。搜索者通常不在乎结果的大小写。
        - Unicode规范化。同一字符序列如重音、连写、全半角、繁简等等有不同的表示，需要规范化。
        - 词干提取。有的语言中动词有多种时态、名词有多种复数形式，有时会带“'s”之类，而搜索者通常不在乎这些，需要把它们归一化。
        - 加入同义词。搜索者通常也希望搜索同义词时结果类同。
        - 加入同音词或拼音。搜索者往往会犯拼写错误或者根本不知道一个字怎么写却知道怎么读。
        - 去除停用词。去除太常见的词可节省空间。如英文中常去除“a”、“an”、“and”、“are”、“as”、“at”、“be”、“but”、“by”、“for”、“if”、“in”、“into”、“is”、“it”、“no”、“not”、“of”、“on”、“or”、“such”、“that”、“the”、“their”、“then”、“there”、“these”、“they”、“this”、“to”、“was”、“will”、“with”。不过这样下来搜索“To be or not to be”就没有有意义的结果了，所以往往有另一个保护词名单。
        - 去掉两侧空白。
3. 更新索引。让上一步最后得到词指向文档。不过有的的搜索系统会拒绝与已经索引过的文档高度雷同的文档，这通常是通过增加一个散列值字段来实现的。

在查询时，需要进行以下工作：
1. 解析用户的查询。根据搜索语法和语义理解用户的查询，即结果应满足的条件（如`足球 篮球 -网球 site:cctv.cn`可能被理解为寻找含有“足球”或“篮球“的文档，它的site字段为“cctv.cn”，其中不能含有”网球”），这还可能需要应用类似于与索引时字段分析的方法。
2. 通过倒排索引找出结果集的一个超集（如上例中可能为含“足球”文档集和含“篮球“文档集的并）。
3. 过滤，即从上一步结果集中去除不满足整个查询的结果集（如上例中可能要去除含“网球“文档集和site字段不匹配“cctv.cn”的文档集）。
4. 对结果排序。当搜索结果很多时，用户一般只会注意到排在最前面的结果，所以排序对搜索质量非常重要。
    - 相关度。我们希望与查询最吻合的结果排在最前面。至于相关度的计算有不同方法，为了简单见，我们仅介绍一种基本的方法，基本想法是计算查询中各单词频率与文档中各单词频率的相似性（使用频率而不是频数是因为避免对不同查询都返回几乎什么词都有的长文档），但注意到不同词的重要性是不同的，越罕见的词应该越有区分力。假设索引中有词$k\_1,\dots,k\_n$，在一个并查询中它们的频率分别为$f\_1,\dots,f\_n$，在文档中它们的频率分别为$F\_1,\dots,F\_n$，而在所有文档中它们出现的频率分别为$N\_1,\dots,N\_n$，则记$q=(\log (1+f\_1)\log \frac{1}{N\_1},\dots,\log (1+f\_n)\log \frac{1}{N\_n})$，$d=(\log (1+F\_1)\log \frac{1}{N\_1},\dots,\log (1+F\_n)\log \frac{1}{N\_n})$，于是查询与文档的相关系数为$\frac{q\cdot d}{\vert q\vert\vert d\vert}$。其中$q$、$d$中各分量的计算有不同方法，但都是一个跟局部词频有关的因子（TD）和一个与全局词频有关的因子（IDF）之积。由于查询中词一般很少，公式中非零项很少，所以计算可以很快。
    - 流行度。基于越多人需要的页面也应该是你会需要页面的信念，流行的结果应该排在前面。
        - 入链。基于随机行走模型，用户在进入每个页面后，有一定的概率后退、一定的概率输入一个网址、余下情况则是随机点击页面上的链接，据此可估算用户进入每个页面的概率，这可作为流行度的一个指标，Google当年成名的PageRank算法就在做这事情。
        - 浏览数据。点击率、浏览量、收藏量等等都可反映流行度，许多搜索引擎厂商也做浏览器和公开DNS服务器的部分原因就是要收集这些数据。
    - 字段。对于结构化数据的搜索，根据发布日期、价格等字段排序可能是有意义的。
    - 综合评分。通过把多种指标通过某种公式（如加权平均）结合起来得出一个评分可能可比较完整地反映用户偏好，至于公式及其中参数的选取通常要慢慢试错。
5. 附加工作。对于一个现代的搜索系统，还往往包含以下功能：
    - 搜索建议。建议用户进行另一个查询，这可能基于拼写检查器、关联词（搜索结果中共同出现的其它词）或者协同推荐（其它和你作出过类似查询的人也作了什么查询）
    - 分片。建议用户加上过滤条件，如主题、日期范围等等。
    - 类似结果。需要时用与某个文档的相关度排序结果，在电子商务网站中常见的“类似产品”就是例子。
    - 结果高亮。搜索结果摘要中突出显示出现的搜索关键词，以便让用户理解为什么系统会提供这结果。
    - 结果聚类。可以根据字段或聚类算法把类似结果放在一起以便用户筛选，例如把来自同一问答类网站（如百度知道或Stackoverflow）结果放在一起，又或把对应一个词不同含义的结果分开（如在“IDE”的结果页把关于“Integrated development environment“和关于”Integrated Drive Electronics”的结果分开）。
    - 定制结果。对于特定类型的查询，可能可设计专门的结果，如显示来自维基百科的摘要、加上竞价广告、搜索算式时返回计算结果。

## 配置Solr集合

```
bin/solr zk upconfig -d fulltext_configs -n fulltext_configs -z localhost:9983
```

## 模式

Solr把文档看作由一些字段组成，每个字段可以有不同的类型，以便Solr作出最贴切的处理。通常模式在`managed-schema`文件中指定，在默认配置中它开首形如：

```xml
<?xml version="1.0" encoding="UTF-8" ?>
<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<!--

 本例子是用户的推荐起点，它应当保持准确和精简，直接可用。


 关于定制本文件的更多信息，参见：
 http://lucene.apache.org/solr/guide/documents-fields-and-schema-design.html

 性能注记: 本模式包括许多可选的特性，不应用作性能基准。为提升性能应当：
  - 对于只需搜索而不需要返回原始值的字段（特别是长字段）尽可能设为 stored="false"
  - 对于只需要返回而不需搜索的字段应设为indexed="false"
  - 移除所有不必要的copyField标签
  - 为了索引大小和搜索性能，对一般文本字段设置"index"为false，用copyField复制到
    用于搜索的"text"字段
-->

<schema name="default-config" version="1.6">
    <!-- "name"属性是模式只用于展示，version="x.y"是Solr的模式语法版本号，一般不应修改

       1.0: multiValued属性不存在，所有字段本质上为multiValued 
       1.1: multiValued属性被引入，默认为false 
       1.2: omitTermFreqAndPositions属性被引入，除文本字段外默认为true
       1.3: 移除可选字段compress
       1.4: autoGeneratePhraseQueries属性被引入，默认为off
       1.5: omitNorms对基本字段类型（如int, float, boolean, string）默认为true
       1.6: useDocValuesAsStored默认为true.
    -->
```

### 声明字段

```xml
    <!-- 适用于字段的属性:
     name: 必备 - 字段名
     type: 必备 - 字段类型名，参考fieldTypes节
     indexed: 字段应被索引（用于搜索或排序）时为true
     stored: 字段应能获取时为true
     docValues: 字段应该有文档值时为true。对于分片、分组、排序和函数查询推荐文档值
       (Point字段必备)。文档值使索引加载更快、更近实时友好、内存效率更高。
       目前只支持字符串字段、UUID字段、点字段，部分字段类型可能还要求单值或有不能缺失
     multiValued: 一个文档中能有多个同名字段时为true
     omitNorms: (专家) 设置为true会忽略字段的范数（从而不能长度归一化和索引时提升
       但节省内存），只有全文字段或需要索引时提升的字段需要它。
       对于基本 (不分析的) 类型默认忽略范数
     termVectors: [false] 设置为true会保存字段的项向量，有利于提升作为
       MoreLikeThis相似度字段时的性能
     termPositions: 把位置信息保存到项向量，会增加空间开销
     termOffsets: 把偏移信息保存到项向量，会增加空间开销
     required: 字段是必备的，缺失时抛出错误
     default: 增加文档时字段缺失时用来补上的默认值
    -->

    <!-- 字段名应当由字母、数字或下划线组成，不以数字开始。这目前不是强制的，
      但其它字段名可能不会被所有组件原生支持，向后兼容性也没有保证。
      同时由下划线开首和结束的名字 (如 _version_) 被保留
    -->

    <!-- 在这个_default配置集，只预定义了四个字段id, _version_,  _text_ 和 _root_
         所有其它字段会通过类型猜测和solrconfig.xml声明的"add-unknown-fields-to-the-schema"
         更新处理器链声明
         
         注意定义了许多动态字段 - 你可以用它们通过命名约定指定字段类型 - 如下.
  
         警告: 捕获字段 _text_ 会显著增大索引大小，不需要的话移除它及对应的copyField标签
    -->

    <field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" />
    <!-- docValues对long类型默认启用，所以不用索引版本字段  -->
    <field name="_version_" type="plong" indexed="false" stored="false"/>
    <field name="_root_" type="string" indexed="true" stored="false" docValues="false" />
    <field name="_text_" type="text_general" indexed="true" stored="false" multiValued="true"/>
    <!-- 在客户不知道该搜索哪些字段时可以启用它，但索引所有东西再次是昂贵的 -->
    <!-- <copyField source="*" dest="_text_"/> -->

    <!-- 动态字段容许通过字段名模式对字段约定先于配置
       例如:  name="*_i" 会匹配所有以 _i 结束的字段名 (如 myid_i, z_i)
       限制: 类glob模式的name属性中"*"只能在开始或结束  -->
   
    <dynamicField name="*_i"  type="pint"    indexed="true"  stored="true"/>
    <dynamicField name="*_is" type="pints"    indexed="true"  stored="true"/>
    <dynamicField name="*_s"  type="string"  indexed="true"  stored="true" />
    <dynamicField name="*_ss" type="strings"  indexed="true"  stored="true"/>
    <dynamicField name="*_l"  type="plong"   indexed="true"  stored="true"/>
    <dynamicField name="*_ls" type="plongs"   indexed="true"  stored="true"/>
    <dynamicField name="*_t" type="text_general" indexed="true" stored="true" multiValued="false"/>
    <dynamicField name="*_txt" type="text_general" indexed="true" stored="true"/>
    <dynamicField name="*_b"  type="boolean" indexed="true" stored="true"/>
    <dynamicField name="*_bs" type="booleans" indexed="true" stored="true"/>
    <dynamicField name="*_f"  type="pfloat"  indexed="true"  stored="true"/>
    <dynamicField name="*_fs" type="pfloats"  indexed="true"  stored="true"/>
    <dynamicField name="*_d"  type="pdouble" indexed="true"  stored="true"/>
    <dynamicField name="*_ds" type="pdoubles" indexed="true"  stored="true"/>

    <!-- 用于数据驱动模式的类型，为每个文本字段增加一个字符串副本 -->
    <dynamicField name="*_str" type="strings" stored="false" docValues="true" indexed="false" />

    <dynamicField name="*_dt"  type="pdate"    indexed="true"  stored="true"/>
    <dynamicField name="*_dts" type="pdate"    indexed="true"  stored="true" multiValued="true"/>
    <dynamicField name="*_p"  type="location" indexed="true" stored="true"/>
    <dynamicField name="*_srpt"  type="location_rpt" indexed="true" stored="true"/>
    
    <!-- 负载动态字段 -->
    <dynamicField name="*_dpf" type="delimited_payloads_float" indexed="true"  stored="true"/>
    <dynamicField name="*_dpi" type="delimited_payloads_int" indexed="true"  stored="true"/>
    <dynamicField name="*_dps" type="delimited_payloads_string" indexed="true"  stored="true"/>

    <dynamicField name="attr_*" type="text_general" indexed="true" stored="true" multiValued="true"/>

    <!-- 用于强制文档惟一性的字段，除非标记为 required="false"，否则是必备的 -->
    <uniqueKey>id</uniqueKey>

    <!-- copyField 在文档增加到索引时把一个字段复制到另一个，以便用不同方式索引相同数据
       或者把多个字段复制到一个以便更容易或快捷的搜索

    <copyField source="sourceFieldName" dest="destinationFieldName"/>
    -->
```

对于不同应用，需要声明不同的字段，例如一个全文搜索PDF、HTML、Microsoft Office、Open Document等非结构化格式的文档的搜索系统可能需要如下声明字段：

```xml
    <field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" />
    <field name="_version_" type="plong" indexed="false" stored="false"/>
    <field name="_root_" type="string" indexed="true" stored="false" docValues="false" />
    <field name="_text_" type="text_general" indexed="true" stored="false" multiValued="true"/>

    <field name="content_type" type="string" indexed="true" stored="true"/>
    <field name="language" type="string" indexed="true" stored="true"/>
    <field name="url" type="string" indexed="true" stored="true"/>
    <field name="date" type="pdates" indexed="true" stored="true"/>
    <field name="keywords" type="text_general" indexed="true" stored="true" multiValued="true"/>
    <field name="subject" type="text_general" indexed="true" stored="true" multiValued="true"/>
    <field name="description" type="text_general" indexed="true" stored="true"/>
    <field name="author" type="text_general" indexed="true" stored="true" multiValued="true"/>
    <field name="title" type="text_general" indexed="true" stored="true"/>
    <field name="content" type="text_general" indexed="true" stored="true"/>

    <copyField source="content_type" dest="_text_"/>
    <copyField source="language" dest="_text_"/>
    <copyField source="url" dest="_text_"/>
    <copyField source="date" dest="_text_"/>
    <copyField source="keywords" dest="_text_"/>
    <copyField source="subject" dest="_text_"/>
    <copyField source="description" dest="_text_"/>
    <copyField source="author" dest="_text_"/>
    <copyField source="title" dest="_text_"/>
    <copyField source="content" dest="_text_"/>
```

### 声明字段类型


```xml
    <!-- 字段类型定义。"name"属性只用作标签。"class"属性和其它属性决定字段类型的行为
         由"solr"开始的类名在org.apache.solr.analysis包中
    -->

    <!-- sortMissingLast 和 sortMissingFirst 属性目前只被内部排序为字符串或数值的类型支持。
       包括"string", "boolean", "pint", "pfloat", "plong", "pdate", "pdouble"。
       - 若sortMissingLast="true"，则缺失字段的文档排到后面，不管排序方向
       - 若sortMissingFirst="true"，则缺失字段的文档排到前面，不管排序方向
       - 若sortMissingLast="false"且sortMissingFirst="false" (默认),
         则顺序排序时缺失字段文档在前面而逆序排序时缺失字段文档在后面
    -->

    <!-- StrField不被分析，会完好地被索引或存储 -->
    <fieldType name="string" class="solr.StrField" sortMissingLast="true" docValues="true" />
    <fieldType name="strings" class="solr.StrField" sortMissingLast="true" multiValued="true" docValues="true" />

    <!-- 布尔类型: "true" 或 "false" -->
    <fieldType name="boolean" class="solr.BoolField" sortMissingLast="true"/>
    <fieldType name="booleans" class="solr.BoolField" sortMissingLast="true" multiValued="true"/>

    <!--
      用KD树索引值的数值字段
      点字段不支持字段缓存FieldCache，所以必须docValues="true"时才能用于排序、分片、函数等等
    -->
    <fieldType name="pint" class="solr.IntPointField" docValues="true"/>
    <fieldType name="pfloat" class="solr.FloatPointField" docValues="true"/>
    <fieldType name="plong" class="solr.LongPointField" docValues="true"/>
    <fieldType name="pdouble" class="solr.DoublePointField" docValues="true"/>
    
    <fieldType name="pints" class="solr.IntPointField" docValues="true" multiValued="true"/>
    <fieldType name="pfloats" class="solr.FloatPointField" docValues="true" multiValued="true"/>
    <fieldType name="plongs" class="solr.LongPointField" docValues="true" multiValued="true"/>
    <fieldType name="pdoubles" class="solr.DoublePointField" docValues="true" multiValued="true"/>

    <!-- 日期字段格式如 1995-12-31T23:59:59Z, 是比日期时间规范表示更受限的形式
         http://www.w3.org/TR/xmlschema-2/#dateTime    
         最后的 "Z" 表示 UTC 时间且为强制的。
         容许可选可选的小数秒: 1995-12-31T23:59:59.999Z
         所有其它部分为强制的。

         表达式中容许基于"NOW"计算时间, 如

               NOW/HOUR
                  ... 表示舍入到当前小时
               NOW-1DAY
                  ... 表示1天前
               NOW/DAY+6MONTHS+3DAYS
                  ... 表示6个月又3日后
                      
      -->
    <!-- 日期字段的KD-树版本 -->
    <fieldType name="pdate" class="solr.DatePointField" docValues="true"/>
    <fieldType name="pdates" class="solr.DatePointField" docValues="true" multiValued="true"/>
    
    <!-- 二进制数值类型。用Base64编码的字符串发送和接收 -->
    <fieldType name="binary" class="solr.BinaryField"/>

    <!-- solr.TextField容许指定定制的分析器为一个分词器和一些过滤器。
         索引和查询时可以使用不同分析器。

         可选的positionIncrementGap把空白加到同一文档的多个字段间以避免假词组。

         关于定制分析器链的更多信息参见
         http://lucene.apache.org/solr/guide/understanding-analyzers-tokenizers-and-filters.html#understanding-analyzers-tokenizers-and-filters
     -->

    <!-- 可以通过analyzer元素的class属性指定一个有默认构造器的分析器类，如：
    <fieldType name="text_greek" class="solr.TextField">
      <analyzer class="org.apache.lucene.analysis.el.GreekAnalyzer"/>
    </fieldType>
    -->

    <!-- 一个在空白处切开的文本字段，用于单词的精确匹配 -->
    <dynamicField name="*_ws" type="text_ws"  indexed="true"  stored="true"/>
    <fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
      </analyzer>
    </fieldType>

    <!-- 一般是文本字段，对跨语言有合理和泛化的默认: 用 StandardTokenizer分词,
	       从大小写无关的表"stopwords.txt"(默认为空)去除停用词, 小写化。
               只在查询时用同义词。
	  -->
    <fieldType name="text_general" class="solr.TextField" positionIncrementGap="100" multiValued="true">
      <analyzer type="index">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <!-- in this example, we will only use synonyms at query time
        <filter class="solr.SynonymGraphFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
        <filter class="solr.FlattenGraphFilterFactory"/>
        -->
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.SynonymGraphFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    
    <!-- SortableTextField与TextField类似, 但它支持并默认用docValues来基于前1024（可配置）字符排序和分片。
         
         这使它在许多情况比TextField更有用，但占用更多磁盘空间; 这就是它们都存在的原因
	  -->
    <dynamicField name="*_t_sort" type="text_gen_sort" indexed="true" stored="true" multiValued="false"/>
    <dynamicField name="*_txt_sort" type="text_gen_sort" indexed="true" stored="true"/>
    <fieldType name="text_gen_sort" class="solr.SortableTextField" positionIncrementGap="100" multiValued="true">
      <analyzer type="index">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.SynonymGraphFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- 默认适合英文的文本字段: 用StandardTokenizer分词, 去除英文停用词 (lang/stopwords_en.txt), 
         小写化、保护protwords.txt中词, 最后用Porter的词干提取。查询时支持同义词 -->
    <dynamicField name="*_txt_en" type="text_en"  indexed="true"  stored="true"/>
    <fieldType name="text_en" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- in this example, we will only use synonyms at query time
        <filter class="solr.SynonymGraphFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
        <filter class="solr.FlattenGraphFilterFactory"/>
        -->
        <!-- Case insensitive stop word removal.
        -->
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_en.txt"
            />
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.EnglishPossessiveFilterFactory"/>
        <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
        <!-- Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:
        <filter class="solr.EnglishMinimalStemFilterFactory"/>
	      -->
        <filter class="solr.PorterStemFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.SynonymGraphFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_en.txt"
        />
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.EnglishPossessiveFilterFactory"/>
        <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
        <!-- Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:
        <filter class="solr.EnglishMinimalStemFilterFactory"/>
	      -->
        <filter class="solr.PorterStemFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- 默认适合英文的文本字段，加上积极的分词和自动词组特性。
         与text_en类似，除了加入WordDelimiterGraphFilter以
         启用在大小写变化时、字母数值边界、非字母数值边界分词和匹配。
         这意味着"wi fi"匹配"WiFi"或"wi-fi".
    -->
    <dynamicField name="*_txt_en_split" type="text_en_splitting"  indexed="true"  stored="true"/>
    <fieldType name="text_en_splitting" class="solr.TextField" positionIncrementGap="100" autoGeneratePhraseQueries="true">
      <analyzer type="index">
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <!-- in this example, we will only use synonyms at query time
        <filter class="solr.SynonymGraphFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
        -->
        <!-- Case insensitive stop word removal.
        -->
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_en.txt"
        />
        <filter class="solr.WordDelimiterGraphFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
        <filter class="solr.PorterStemFilterFactory"/>
        <filter class="solr.FlattenGraphFilterFactory" />
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.SynonymGraphFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="lang/stopwords_en.txt"
        />
        <filter class="solr.WordDelimiterGraphFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="1"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
        <filter class="solr.PorterStemFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- 较不灵活的匹配，但更少伪匹配。可能不适合产品名，但可能适合SKUs。可在错误位置插入横线而仍然匹配 -->
    <dynamicField name="*_txt_en_split_tight" type="text_en_splitting_tight"  indexed="true"  stored="true"/>
    <fieldType name="text_en_splitting_tight" class="solr.TextField" positionIncrementGap="100" autoGeneratePhraseQueries="true">
      <analyzer type="index">
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.SynonymGraphFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="false"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_en.txt"/>
        <filter class="solr.WordDelimiterGraphFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
        <filter class="solr.EnglishMinimalStemFilterFactory"/>
        <!-- this filter can remove any duplicate tokens that appear at the same position - sometimes
             possible with WordDelimiterGraphFilter in conjuncton with stemming. -->
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
        <filter class="solr.FlattenGraphFilterFactory" />
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.SynonymGraphFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="false"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_en.txt"/>
        <filter class="solr.WordDelimiterGraphFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
        <filter class="solr.EnglishMinimalStemFilterFactory"/>
        <!-- this filter can remove any duplicate tokens that appear at the same position - sometimes
             possible with WordDelimiterGraphFilter in conjuncton with stemming. -->
        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- 与text_general类似但它反转每个单词的字符以便让前方通配更高效。
    -->
    <dynamicField name="*_txt_rev" type="text_general_rev"  indexed="true"  stored="true"/>
    <fieldType name="text_general_rev" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.ReversedWildcardFilterFactory" withOriginal="true"
                maxPosAsterisk="3" maxPosQuestion="2" maxFractionAsterisk="0.33"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.SynonymGraphFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <dynamicField name="*_phon_en" type="phonetic_en"  indexed="true"  stored="true"/>
    <fieldType name="phonetic_en" stored="false" indexed="true" class="solr.TextField" >
      <analyzer>
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.DoubleMetaphoneFilterFactory" inject="false"/>
      </analyzer>
    </fieldType>

    <!-- 把整个字段值小写化并保持为完整单元  -->
    <dynamicField name="*_s_lower" type="lowercase"  indexed="true"  stored="true"/>
    <fieldType name="lowercase" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.KeywordTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory" />
      </analyzer>
    </fieldType>

    <!-- 
      在索引时用PathHierarchyTokenizerFactory的例子，从而路径查询匹配一路径或其后代
    -->
    <dynamicField name="*_descendent_path" type="descendent_path"  indexed="true"  stored="true"/>
    <fieldType name="descendent_path" class="solr.TextField">
      <analyzer type="index">
        <tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" />
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.KeywordTokenizerFactory" />
      </analyzer>
    </fieldType>

    <!--
      在查询时用PathHierarchyTokenizerFactory的例子，从而路径查询匹配一路径或其祖先
    -->
    <dynamicField name="*_ancestor_path" type="ancestor_path"  indexed="true"  stored="true"/>
    <fieldType name="ancestor_path" class="solr.TextField">
      <analyzer type="index">
        <tokenizer class="solr.KeywordTokenizerFactory" />
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" />
      </analyzer>
    </fieldType>

    <!-- 这类型把坐标索引为不同子字段。若定义了subFieldType，它指向一个类型和
      一个动态字段定义，匹配 *___<类型名>.  或者若定义了subFieldSuffix
      例如: 如果 subFieldType="double", 则坐标会索引到字段 myloc_0___double,myloc_1___double.
      例如: 如果 subFieldSuffix="_d"，则坐标索引到字段myloc_0_d,myloc_1_d
      子字段是字段类型的实现细节，最终用户不用了解它
     -->
    <dynamicField name="*_point" type="point"  indexed="true"  stored="true"/>
    <fieldType name="point" class="solr.PointType" dimension="2" subFieldSuffix="_d"/>

    <!-- 用于地理搜索过滤和距离排序 -->
    <fieldType name="location" class="solr.LatLonPointSpatialField" docValues="true"/>

    <!-- 支持多值和多边形的地理字段类型，更多信息参见
      http://lucene.apache.org/solr/guide/spatial-search.html
    -->
    <fieldType name="location_rpt" class="solr.SpatialRecursivePrefixTreeFieldType"
               geo="true" distErrPct="0.025" maxDistErr="0.001" distanceUnits="kilometers" />

    <!-- Payloaded field types -->
    <fieldType name="delimited_payloads_float" stored="false" indexed="true" class="solr.TextField">
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.DelimitedPayloadTokenFilterFactory" encoder="float"/>
      </analyzer>
    </fieldType>
    <fieldType name="delimited_payloads_int" stored="false" indexed="true" class="solr.TextField">
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.DelimitedPayloadTokenFilterFactory" encoder="integer"/>
      </analyzer>
    </fieldType>
    <fieldType name="delimited_payloads_string" stored="false" indexed="true" class="solr.TextField">
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.DelimitedPayloadTokenFilterFactory" encoder="identity"/>
      </analyzer>
    </fieldType>

    <!-- 一个语言的例子 (一般按ISO代码顺序) -->

    <!-- 阿拉伯文 -->
    <dynamicField name="*_txt_ar" type="text_ar"  indexed="true"  stored="true"/>
    <fieldType name="text_ar" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- for any non-arabic -->
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_ar.txt" />
        <!-- normalizes ﻯ to ﻱ, etc -->
        <filter class="solr.ArabicNormalizationFilterFactory"/>
        <filter class="solr.ArabicStemFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- 保加利亚文 -->
    <dynamicField name="*_txt_bg" type="text_bg"  indexed="true"  stored="true"/>
    <fieldType name="text_bg" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/> 
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_bg.txt" /> 
        <filter class="solr.BulgarianStemFilterFactory"/>       
      </analyzer>
    </fieldType>
    
    <!-- 加泰罗尼亚文 -->
    <dynamicField name="*_txt_ca" type="text_ca"  indexed="true"  stored="true"/>
    <fieldType name="text_ca" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- removes l', etc -->
        <filter class="solr.ElisionFilterFactory" ignoreCase="true" articles="lang/contractions_ca.txt"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_ca.txt" />
        <filter class="solr.SnowballPorterFilterFactory" language="Catalan"/>       
      </analyzer>
    </fieldType>
    
    <!-- 中日韩2-gram (日文形态学分析配置见text_ja -->
    <dynamicField name="*_txt_cjk" type="text_cjk"  indexed="true"  stored="true"/>
    <fieldType name="text_cjk" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- normalize width before bigram, as e.g. half-width dakuten combine  -->
        <filter class="solr.CJKWidthFilterFactory"/>
        <!-- for any non-CJK -->
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.CJKBigramFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- 捷克文 -->
    <dynamicField name="*_txt_cz" type="text_cz"  indexed="true"  stored="true"/>
    <fieldType name="text_cz" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_cz.txt" />
        <filter class="solr.CzechStemFilterFactory"/>       
      </analyzer>
    </fieldType>
    
    <!-- 丹麦文 -->
    <dynamicField name="*_txt_da" type="text_da"  indexed="true"  stored="true"/>
    <fieldType name="text_da" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_da.txt" format="snowball" />
        <filter class="solr.SnowballPorterFilterFactory" language="Danish"/>       
      </analyzer>
    </fieldType>
    
    <!-- 德文 -->
    <dynamicField name="*_txt_de" type="text_de"  indexed="true"  stored="true"/>
    <fieldType name="text_de" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_de.txt" format="snowball" />
        <filter class="solr.GermanNormalizationFilterFactory"/>
        <filter class="solr.GermanLightStemFilterFactory"/>
        <!-- less aggressive: <filter class="solr.GermanMinimalStemFilterFactory"/> -->
        <!-- more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="German2"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 希腊文 -->
    <dynamicField name="*_txt_el" type="text_el"  indexed="true"  stored="true"/>
    <fieldType name="text_el" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- greek specific lowercase for sigma -->
        <filter class="solr.GreekLowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="false" words="lang/stopwords_el.txt" />
        <filter class="solr.GreekStemFilterFactory"/>
      </analyzer>
    </fieldType>
    
    <!-- 西班牙文 -->
    <dynamicField name="*_txt_es" type="text_es"  indexed="true"  stored="true"/>
    <fieldType name="text_es" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_es.txt" format="snowball" />
        <filter class="solr.SpanishLightStemFilterFactory"/>
        <!-- more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="Spanish"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 巴斯克文 -->
    <dynamicField name="*_txt_eu" type="text_eu"  indexed="true"  stored="true"/>
    <fieldType name="text_eu" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_eu.txt" />
        <filter class="solr.SnowballPorterFilterFactory" language="Basque"/>
      </analyzer>
    </fieldType>
    
    <!-- 波斯文 -->
    <dynamicField name="*_txt_fa" type="text_fa"  indexed="true"  stored="true"/>
    <fieldType name="text_fa" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <!-- for ZWNJ -->
        <charFilter class="solr.PersianCharFilterFactory"/>
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.ArabicNormalizationFilterFactory"/>
        <filter class="solr.PersianNormalizationFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_fa.txt" />
      </analyzer>
    </fieldType>
    
    <!-- 芬兰文 -->
    <dynamicField name="*_txt_fi" type="text_fi"  indexed="true"  stored="true"/>
    <fieldType name="text_fi" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_fi.txt" format="snowball" />
        <filter class="solr.SnowballPorterFilterFactory" language="Finnish"/>
        <!-- less aggressive: <filter class="solr.FinnishLightStemFilterFactory"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 法文 -->
    <dynamicField name="*_txt_fr" type="text_fr"  indexed="true"  stored="true"/>
    <fieldType name="text_fr" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- removes l', etc -->
        <filter class="solr.ElisionFilterFactory" ignoreCase="true" articles="lang/contractions_fr.txt"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_fr.txt" format="snowball" />
        <filter class="solr.FrenchLightStemFilterFactory"/>
        <!-- less aggressive: <filter class="solr.FrenchMinimalStemFilterFactory"/> -->
        <!-- more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="French"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 爱尔兰文 -->
    <dynamicField name="*_txt_ga" type="text_ga"  indexed="true"  stored="true"/>
    <fieldType name="text_ga" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- removes d', etc -->
        <filter class="solr.ElisionFilterFactory" ignoreCase="true" articles="lang/contractions_ga.txt"/>
        <!-- removes n-, etc. position increments is intentionally false! -->
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/hyphenations_ga.txt"/>
        <filter class="solr.IrishLowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_ga.txt"/>
        <filter class="solr.SnowballPorterFilterFactory" language="Irish"/>
      </analyzer>
    </fieldType>
    
    <!-- 加利西亚文 -->
    <dynamicField name="*_txt_gl" type="text_gl"  indexed="true"  stored="true"/>
    <fieldType name="text_gl" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_gl.txt" />
        <filter class="solr.GalicianStemFilterFactory"/>
        <!-- less aggressive: <filter class="solr.GalicianMinimalStemFilterFactory"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 印地文 -->
    <dynamicField name="*_txt_hi" type="text_hi"  indexed="true"  stored="true"/>
    <fieldType name="text_hi" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <!-- normalizes unicode representation -->
        <filter class="solr.IndicNormalizationFilterFactory"/>
        <!-- normalizes variation in spelling -->
        <filter class="solr.HindiNormalizationFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_hi.txt" />
        <filter class="solr.HindiStemFilterFactory"/>
      </analyzer>
    </fieldType>
    
    <!-- 匈牙利文 -->
    <dynamicField name="*_txt_hu" type="text_hu"  indexed="true"  stored="true"/>
    <fieldType name="text_hu" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_hu.txt" format="snowball" />
        <filter class="solr.SnowballPorterFilterFactory" language="Hungarian"/>
        <!-- less aggressive: <filter class="solr.HungarianLightStemFilterFactory"/> -->   
      </analyzer>
    </fieldType>
    
    <!-- 亚美尼亚文 -->
    <dynamicField name="*_txt_hy" type="text_hy"  indexed="true"  stored="true"/>
    <fieldType name="text_hy" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_hy.txt" />
        <filter class="solr.SnowballPorterFilterFactory" language="Armenian"/>
      </analyzer>
    </fieldType>
    
    <!-- 印尼文 -->
    <dynamicField name="*_txt_id" type="text_id"  indexed="true"  stored="true"/>
    <fieldType name="text_id" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_id.txt" />
        <!-- for a less aggressive approach (only inflectional suffixes), set stemDerivational to false -->
        <filter class="solr.IndonesianStemFilterFactory" stemDerivational="true"/>
      </analyzer>
    </fieldType>
    
    <!-- 意大利文 -->
  <dynamicField name="*_txt_it" type="text_it"  indexed="true"  stored="true"/>
  <fieldType name="text_it" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <!-- removes l', etc -->
        <filter class="solr.ElisionFilterFactory" ignoreCase="true" articles="lang/contractions_it.txt"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_it.txt" format="snowball" />
        <filter class="solr.ItalianLightStemFilterFactory"/>
        <!-- more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="Italian"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 基于形态学日文分析 (2-gram配置见text_cjk)

         注记: 如果你想优化精度，在请求处理器用AND作为默认操作符 (q.op) 
         在优化召回率时则用默认的OR
    -->
    <dynamicField name="*_txt_ja" type="text_ja"  indexed="true"  stored="true"/>
    <fieldType name="text_ja" class="solr.TextField" positionIncrementGap="100" autoGeneratePhraseQueries="false">
      <analyzer>
        <!-- Kuromoji Japanese morphological analyzer/tokenizer (JapaneseTokenizer)

           Kuromoji has a search mode (default) that does segmentation useful for search.  A heuristic
           is used to segment compounds into its parts and the compound itself is kept as synonym.

           Valid values for attribute mode are:
              normal: regular segmentation
              search: segmentation useful for search with synonyms compounds (default)
            extended: same as search mode, but unigrams unknown words (experimental)

           For some applications it might be good to use search mode for indexing and normal mode for
           queries to reduce recall and prevent parts of compounds from being matched and highlighted.
           Use <analyzer type="index"> and <analyzer type="query"> for this and mode normal in query.

           Kuromoji also has a convenient user dictionary feature that allows overriding the statistical
           model with your own entries for segmentation, part-of-speech tags and readings without a need
           to specify weights.  Notice that user dictionaries have not been subject to extensive testing.

           User dictionary attributes are:
                     userDictionary: user dictionary filename
             userDictionaryEncoding: user dictionary encoding (default is UTF-8)

           See lang/userdict_ja.txt for a sample user dictionary file.

           Punctuation characters are discarded by default.  Use discardPunctuation="false" to keep them.
        -->
        <tokenizer class="solr.JapaneseTokenizerFactory" mode="search"/>
        <!--<tokenizer class="solr.JapaneseTokenizerFactory" mode="search" userDictionary="lang/userdict_ja.txt"/>-->
        <!-- Reduces inflected verbs and adjectives to their base/dictionary forms (辞書形) -->
        <filter class="solr.JapaneseBaseFormFilterFactory"/>
        <!-- Removes tokens with certain part-of-speech tags -->
        <filter class="solr.JapanesePartOfSpeechStopFilterFactory" tags="lang/stoptags_ja.txt" />
        <!-- Normalizes full-width romaji to half-width and half-width kana to full-width (Unicode NFKC subset) -->
        <filter class="solr.CJKWidthFilterFactory"/>
        <!-- Removes common tokens typically not useful for search, but have a negative effect on ranking -->
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_ja.txt" />
        <!-- Normalizes common katakana spelling variations by removing any last long sound character (U+30FC) -->
        <filter class="solr.JapaneseKatakanaStemFilterFactory" minimumLength="4"/>
        <!-- Lower-cases romaji characters -->
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>
    
    <!-- 拉脱维亚文 -->
    <dynamicField name="*_txt_lv" type="text_lv"  indexed="true"  stored="true"/>
    <fieldType name="text_lv" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_lv.txt" />
        <filter class="solr.LatvianStemFilterFactory"/>
      </analyzer>
    </fieldType>
    
    <!-- 荷兰文 -->
    <dynamicField name="*_txt_nl" type="text_nl"  indexed="true"  stored="true"/>
    <fieldType name="text_nl" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_nl.txt" format="snowball" />
        <filter class="solr.StemmerOverrideFilterFactory" dictionary="lang/stemdict_nl.txt" ignoreCase="false"/>
        <filter class="solr.SnowballPorterFilterFactory" language="Dutch"/>
      </analyzer>
    </fieldType>
    
    <!-- 挪威文 -->
    <dynamicField name="*_txt_no" type="text_no"  indexed="true"  stored="true"/>
    <fieldType name="text_no" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_no.txt" format="snowball" />
        <filter class="solr.SnowballPorterFilterFactory" language="Norwegian"/>
        <!-- less aggressive: <filter class="solr.NorwegianLightStemFilterFactory"/> -->
        <!-- singular/plural: <filter class="solr.NorwegianMinimalStemFilterFactory"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 葡萄牙文 -->
  <dynamicField name="*_txt_pt" type="text_pt"  indexed="true"  stored="true"/>
  <fieldType name="text_pt" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_pt.txt" format="snowball" />
        <filter class="solr.PortugueseLightStemFilterFactory"/>
        <!-- less aggressive: <filter class="solr.PortugueseMinimalStemFilterFactory"/> -->
        <!-- more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="Portuguese"/> -->
        <!-- most aggressive: <filter class="solr.PortugueseStemFilterFactory"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 罗马文 -->
    <dynamicField name="*_txt_ro" type="text_ro"  indexed="true"  stored="true"/>
    <fieldType name="text_ro" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_ro.txt" />
        <filter class="solr.SnowballPorterFilterFactory" language="Romanian"/>
      </analyzer>
    </fieldType>
    
    <!-- 俄文 -->
    <dynamicField name="*_txt_ru" type="text_ru"  indexed="true"  stored="true"/>
    <fieldType name="text_ru" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_ru.txt" format="snowball" />
        <filter class="solr.SnowballPorterFilterFactory" language="Russian"/>
        <!-- less aggressive: <filter class="solr.RussianLightStemFilterFactory"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 瑞典文 -->
    <dynamicField name="*_txt_sv" type="text_sv"  indexed="true"  stored="true"/>
    <fieldType name="text_sv" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_sv.txt" format="snowball" />
        <filter class="solr.SnowballPorterFilterFactory" language="Swedish"/>
        <!-- less aggressive: <filter class="solr.SwedishLightStemFilterFactory"/> -->
      </analyzer>
    </fieldType>
    
    <!-- 泰文 -->
    <dynamicField name="*_txt_th" type="text_th"  indexed="true"  stored="true"/>
    <fieldType name="text_th" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.ThaiTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_th.txt" />
      </analyzer>
    </fieldType>
    
    <!-- 土耳其文 -->
    <dynamicField name="*_txt_tr" type="text_tr"  indexed="true"  stored="true"/>
    <fieldType name="text_tr" class="solr.TextField" positionIncrementGap="100">
      <analyzer> 
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.TurkishLowerCaseFilterFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="false" words="lang/stopwords_tr.txt" />
        <filter class="solr.SnowballPorterFilterFactory" language="Turkish"/>
      </analyzer>
    </fieldType>

    <!-- 相似性用于评价每个文档对查询的评分
       这里可指定定制的Similarity或SimilarityFactory，但默认的对大多数应用良好
       更多信息: http://lucene.apache.org/solr/guide/other-schema-elements.html#OtherSchemaElements-Similarity
    -->
    <!--
     <similarity class="com.example.solr.CustomSimilarityFactory">
       <str name="paramkey">param value</str>
     </similarity>
    -->

</schema>
```



### 无模式模式

不指定模式时，Solr可以自动猜测字段的类型并加到模式中。对于默认的`solrconfig.xml`，应该有这段：

```xml
  <updateProcessor class="solr.UUIDUpdateProcessorFactory" name="uuid"/>
  <updateProcessor class="solr.RemoveBlankFieldUpdateProcessorFactory" name="remove-blank"/>
  <updateProcessor class="solr.FieldNameMutatingUpdateProcessorFactory" name="field-name-mutating">
    <str name="pattern">[^\w-\.]</str>
    <str name="replacement">_</str>
  </updateProcessor>
  <updateProcessor class="solr.ParseBooleanFieldUpdateProcessorFactory" name="parse-boolean"/>
  <updateProcessor class="solr.ParseLongFieldUpdateProcessorFactory" name="parse-long"/>
  <updateProcessor class="solr.ParseDoubleFieldUpdateProcessorFactory" name="parse-double"/>
  <updateProcessor class="solr.ParseDateFieldUpdateProcessorFactory" name="parse-date">
    <arr name="format">
      <str>yyyy-MM-dd'T'HH:mm:ss.SSSZ</str>
      <str>yyyy-MM-dd'T'HH:mm:ss,SSSZ</str>
      <str>yyyy-MM-dd'T'HH:mm:ss.SSS</str>
      <str>yyyy-MM-dd'T'HH:mm:ss,SSS</str>
      <str>yyyy-MM-dd'T'HH:mm:ssZ</str>
      <str>yyyy-MM-dd'T'HH:mm:ss</str>
      <str>yyyy-MM-dd'T'HH:mmZ</str>
      <str>yyyy-MM-dd'T'HH:mm</str>
      <str>yyyy-MM-dd HH:mm:ss.SSSZ</str>
      <str>yyyy-MM-dd HH:mm:ss,SSSZ</str>
      <str>yyyy-MM-dd HH:mm:ss.SSS</str>
      <str>yyyy-MM-dd HH:mm:ss,SSS</str>
      <str>yyyy-MM-dd HH:mm:ssZ</str>
      <str>yyyy-MM-dd HH:mm:ss</str>
      <str>yyyy-MM-dd HH:mmZ</str>
      <str>yyyy-MM-dd HH:mm</str>
      <str>yyyy-MM-dd</str>
    </arr>
  </updateProcessor>
  <updateProcessor class="solr.AddSchemaFieldsUpdateProcessorFactory" name="add-schema-fields">
    <lst name="typeMapping">
      <str name="valueClass">java.lang.String</str>
      <str name="fieldType">text_general</str>
      <lst name="copyField">
        <str name="dest">*_str</str>
        <int name="maxChars">256</int>
      </lst>
      <!-- Use as default mapping instead of defaultFieldType -->
      <bool name="default">true</bool>
    </lst>
    <lst name="typeMapping">
      <str name="valueClass">java.lang.Boolean</str>
      <str name="fieldType">booleans</str>
    </lst>
    <lst name="typeMapping">
      <str name="valueClass">java.util.Date</str>
      <str name="fieldType">pdates</str>
    </lst>
    <lst name="typeMapping">
      <str name="valueClass">java.lang.Long</str>
      <str name="valueClass">java.lang.Integer</str>
      <str name="fieldType">plongs</str>
    </lst>
    <lst name="typeMapping">
      <str name="valueClass">java.lang.Number</str>
      <str name="fieldType">pdoubles</str>
    </lst>
  </updateProcessor>

  <!-- The update.autoCreateFields property can be turned to false to disable schemaless mode -->
  <updateRequestProcessorChain name="add-unknown-fields-to-the-schema" default="${update.autoCreateFields:true}"
           processor="uuid,remove-blank,field-name-mutating,parse-boolean,parse-long,parse-double,parse-date,add-schema-fields">
    <processor class="solr.LogUpdateProcessorFactory"/>
    <processor class="solr.DistributedUpdateProcessorFactory"/>
    <processor class="solr.RunUpdateProcessorFactory"/>
  </updateRequestProcessorChain>
```

它表明除非属性`update.autoCreateFields`被设为`false`，否则在碰到模式没有定义的字段时会根据一定的规则企图解析字段的值，然后把字段加到模式中，并把它视为有对应的类型。值得注意的是，如果以后再企图索引文档遇到同一字段，如果值不符合这里猜测的类型，则会导致错误。

## 用Solr索引数据

## 用Solr搜索

### 查询API

Solr提供了RESTful的API，可以用HTTP的GET方法进行查询，查询和其它参数用GET参数给出即可，例如：

```
curl "http://localhost:8983/solr/集合/query?q=一些东西&fq=inTitle:gov"
```

另一种方式则是把JSON格式的查询以HTTP的POST方法提供，例如：

```
curl http://localhost:8983/solr/techproducts/query -d '
{
"query" : "一些东西",
"filter" : "inTitle:gov"
}'
```

值得指出的是：
- 多次指定单值参数，最后指定的值生效
- 多次指定多值参数，它们会连接起来
- 形如`json.<path>`的请求参数的值会插入到JSON体中合适地方（晚于JSON体被解析）
- 在JSON体中用`query`、`filter`、`offset`、`limit`而非`q`、`fq`、`start`、`rows`

返回的结果通常是JSON格式的，形如：

```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":5,
    "params":{
      "q":"\"基本法\"",
      "fl":"id,date,title,score",
      "_":"1524202118322"}},
  "response":{"numFound":211,"start":0,"maxScore":1.9131931,"docs":[
      {
        "id":"/home/kwong/projects/translate/data/chi/A203!zh-Hant-HK.assist.pdf",
        "date":["2017-02-15T04:09:04Z"],
        "title":["全國人民代表大會關於批准香港特別行政區基本法起草委員會關於設立全國人民代表大會常務委員會香港特別行政區基本法委員會的建議的決定(1990年4月4日第七屆全國人民代表大會第三次會議通過)"],
        "score":1.9131931},
      {
        "id":"/home/kwong/projects/translate/data/chi/A114!zh-Hant-HK.assist.pdf",
        "date":["2017-02-15T04:34:18Z"],
        "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》第十三條第一款和第十九條的解釋(2011年8月26日第十一屆全國人民代表大會常務委員會第二十二次會議通過)"],
        "score":1.9117998},
      {
        "id":"/home/kwong/projects/translate/data/chi/A106!zh-Hant-HK.assist.pdf",
        "date":["2017-02-15T03:16:14Z"],
        "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》第二十二條第四款和第二十四條第二款第(三)項的解釋 (1999年6月26日第九屆全國人民代表大會常務委員會第十次會議通過)"],
        "score":1.9054346},
      {
        "id":"/home/kwong/projects/translate/data/chi/A206!zh-Hant-HK.assist.pdf",
        "date":["2017-06-02T07:57:24Z"],
        "title":["全國人民代表大會常務委員會關於根據《中華人民共和國香港特別行政區基本法》第一百六十條處理香港原有法律的決定(1997年2月23日第八屆全國人民代表大會常務委員會第二十四次會議通過)"],
        "score":1.9049824},
      {
        "id":"/home/kwong/projects/translate/data/chi/A101!zh-Hant-HK.assist.pdf",
        "date":["2018-01-05T04:06:43Z"],
        "title":["中華人民共和國香港特別行政區基本法(1990年4月4日第七屆全國人民代表大會第三次會議通過)"],
        "score":1.904556},
      {
        "id":"/home/kwong/projects/translate/data/chi/A115!zh-Hant-HK.assist.pdf",
        "date":["2017-01-19T10:46:04Z"],
        "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》第一百零四條的解釋"],
        "score":1.9008186},
      {
        "id":"/home/kwong/projects/translate/data/chi/A108!zh-Hant-HK.assist.pdf",
        "date":["2017-02-15T04:15:28Z"],
        "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》第五十三條第二款的解釋(2005年4月27日第十屆全國人民代表大會常務委員會第十五次會議通過)"],
        "score":1.8961817},
      {
        "id":"/home/kwong/projects/translate/data/chi/A212!zh-Hant-HK.assist.pdf",
        "date":["2017-09-06T09:00:47Z"],
        "title":["全國人民代表大會常務委員會關於香港特別行政區行政長官普選問題和 2016 年立法會產生辦法的決定(2014年8月31日第十二屆全國人民代表大會常務委員會第十次會議通過)"],
        "score":1.8927253},
      {
        "id":"/home/kwong/projects/translate/data/chi/A107!zh-Hant-HK.assist.pdf",
        "date":["2017-02-13T08:21:10Z"],
        "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》附件一第七條和附件二第三條的解釋(2004年4月6日第十屆全國人民代表大會常務委員會第八次會議通過)"],
        "score":1.8785428},
      {
        "id":"/home/kwong/projects/translate/data/chi/A104!zh-Hant-HK.assist.pdf",
        "date":["2017-07-05T06:16:00Z"],
        "title":["全國人民代表大會關於《中華人民共和國香港特別行政區基本法》的決定 (1990年4月4日第七屆全國人民代表大會第三次會議通過)"],
        "score":1.8726296}]
  }}
```

参数|用途|默认值
---|---|---
defType|查询解析器，下一小节会进一步介绍|lucene
sort|排序条件列表（用`,`分隔，由重要到不重要），每个条件由量（可以是`score`、`docValues="true"`的基本字段、单值文本字段、或函数结果）、空格和顺序（`asc`或`desc`）|
start|返回的首个结果的序号|0
rows|返回的结果个数|10
fq|附加查询，返回的结果必须满足这查询（但不影响评分），往往比直接写到主查询时更快|
fl|返回结果中包含的字段列表（用`,`分隔，每个字段前可以加`别名:`来引入别名），其中字段必须为`stored="true"`或`docValues="true"`的，另外可用`score`、函数结果或转换器|
debug|返回的调试信息类型（可指定多次）：`query`、`timing`、`results`、`all`（或`true`）|
explainOther|比较查询，会返回它与主查询分别前列结果评分差异的说明|
timeAllowed|限时（毫秒），超时的话结果计数和分片计数等统计数据可能不准确|
segmentTerminateEarly|是否容许提早退出，提早退出的话结果计数和分片计数等统计数据可能不准确|`false`
omitHeader|是否去除返回头字段`responseHeader`|`false`
wt|返回格式化器，可能值取决于`solrconfig.xml`中的`queryResponseWriter`标签|json
cache|是否缓存所有查询（包括过滤查询）结果|`true`
logParamsList|记录到日志的参数列表（用`,`分隔）|全部
echoParams|在返回中显示哪些请求参数，可以是`none`、`explicit`（回显所有显式参数和64位时间戳`_`）、`all`（再显示来自`solrconfig.xml`的隐式参数）| `explicit`

至于上面提到的函数结果可以有以下形式：
- 数值或字符串字面值
- 字段
- 参数引用`${参数}`
- 函数应用
    - `abs(x)`表示`x`的绝对值
    - `childfield(字段,...)`对`{!parent}`查询表示子文档特定字段的值
    - `concat(字符串,...)`表示把字符串连接起来
    - `def(字段,默认值)`表示特定字段的值（没有则默认值）
    - `div(x,y)`表示商
    - `dist(1,x1,...,xn,y1,...,yn)`表示两个向量的马氏距离
    - `dist(2,x1,...,xn,y1,...,yn)`表示两个向量的欧氏距离
    - `docfreq(field,val)`表示特定字段有特定值的文档数
    - `field(字段)`表示特定数值`docValues`或`indexed`字段的值
    - `field(字段,min)`表示特定数值`docValues`字段的最小值
    - `field(字段,max)`表示特定数值`docValues`字段的最大值
    - `hsin(2,布尔值,x1,...,xn,y1,...,yn)`表示球面上两点（用弧度表示）的球面距离，结果是否弧度取决于布尔值
    - `idf(字段,项)`表示全局倒数文档频率
    - `if(布尔值,值1,值2)`在布尔值为真时表示`值1`否则`值2`
    - `linear(x,m,c)`表示mx+c的值
    - `log(x)`表示`x`的常用对数
    - `map(x,min,max,target)`当`x`介于`min`与`max`之间时表示`target`否则表示`x`
    - `map(x,min,max,target,default)`当`x`介于`min`与`max`之间时表示`target`否则表示`default`
    - `max(x,y,...)`表示最大值
    - `maxdoc()`表示索引中总文档数
    - `min(x,y,...)`表示最小值
    - `ms()`相当于`ms(NOW)` , number of milliseconds since the epoch.
    - `ms(时间)`表示从UTC时间1970年1月1日到指定时间经过的毫秒数
    - `ms(时间1,时间2)`表示从`时间2`到`时间1`的毫秒数
    - `norm(字段)`表示字段的索引时提升因子与根据相似性的长度归一化因子的积
    - `numdocs()`表示索引中没有被标记为已删除的文档数
    - `ord(字段)`表示文档按特定字段的排序，从1开始，没有该字段则为0
    - `payload(字段,项)`和下面同，只是默认值为0.0
    - `payload(字段,项,默认值)`和下面同，只是函数为`average`，默认值可以为常数、字段或浮点数函数结果
    - `payload(字段,项,默认值,函数)`表示项的有效负载的总结，没有则表示默认值，其中函数可以为`min`、`max`、`average`或`first`
    - `pow(x,y)`表示幂
    - `product(x,y,...)`或`mul(x,y,...)`表示积
    - `query(子查询,默认值)`表示文档对子查询的评分，不匹配则取默认值
    - `recip(x,m,a,b)`表示a/(mx+b)的值
    - `rord(字段)`表示文档按特定字段的反向序号
    - `scale(x, minTarget, maxTarget)`在`x`大于`maxTarget`时表示`maxTarget`，在`x`小于`minTarget`时表示`minTarget`，否则表示`x`
    - `sqedist(x1,...,xn,y1,...,yn)`表示两个向量的欧氏距离的平方，通常用于节省开方所需的时间
    - `sqrt(x)`表示`x`的平方根
    - `strdist(字符串,字符串,算法)`表示两个字符串间距离，其中算法可以为`jw`（Jaro-Winkler）、`edit`（编辑距离）、`ngram`（N-gram距离，可再加参数`n`，否则默认2-gram）或完全限定类名（有无参构造器）
    - `sub(x,y)`表示两个数的差
    - `sum(x,y,...)`表示一些数之和
    - `sumtotaltermfreq(字段)`或`sttf(字段)`表示索引中特定字段的总项数
    - `termfreq(字段,项)`表示文档中项的出现次数
    - `tf(字段,项)`表示项频因子
    - `top(函数结果)`表示在整个索引中求值其参数
    - `totaltermfreq(字段,项)`或`ttf(字段,项)`表示整个索引中项出现的次数
    - `and(布尔值,...)`表示与
    - `or(布尔值,布尔值)`表示或
    - `xor(布尔值,布尔值)`表示异或
    - `not(布尔值)`表示否定
    - `exists(字段)`表示字段的存在性
    - `gt(x,y)`表示大于
    - `gte(x,y)`表示大于或等于
    - `lt(x,y)`表示小于
    - `lte(x,y)`表示小于或等于
    - `eq(x,y)`表示等于

至于上面提到的转换器可以有以下形式（另外在`solrconfig.xml`中可以用`transformer`标签配置转换器）：
- `[value v=值 t=类型]`表示给定值，类型默认为字符串
- `[explain style=格式]`表示调试信息，其中格式可以为`text`、`html`或`nl`
- `[child parentFilter=祖先过滤查询 childFilter=后代过滤查询 limit=最多返回的后代数]`表示后代文档列表，其中不指定`childFilter`则表示所有后代，不指定`limit`表示10
- `[shard]`表示文档来自的结点
- `[docid]`表示文档的Lucene内部ID
- `[elevated]`表示文档是否被elevator组件置顶（在`elevateIds`参数或组件设置的文件`config-file`中）
- `[excluded]`表示文档是否被elevator组件排除（在`excludeIds`参数）
- `[json]`表示JSON格式的文档（字符串）
- `[xml]`表示XML格式的文档（字符串）
- `[subquery]`表示以文档作子查询得到的文档列表
- `[geo f=字段 w=格式]`表示格式化地理数据，其中格式可以为`WKT`或`GeoJSON`
- `[features]`表示特性的值

有两种方法可以为参数提供局部参数：
- 在参数值前加上形如`{!局部参数=值 局部参数=值 局部参数=值}`
- `{!局部参数=值 局部参数=值 v=主参数值}`

其中值带空格的话可以用单引号或双引号包围，并可以用`${参数}`引用其它参数的值，省略`局部参数=`则把参数视为`type`（用于参数解析器，默认为`lucene`）。

### 搜索语法

以下介绍几种常用的查询语法。

#### 标准语法

当使用标准查询解析器（`defType`为`lucene`）时，查询语法与Lucene接近，功能丰富：
- 查询的基本组成单位是项
    - 单词项由单词组成，其中可以使用能配符：
        - `?`匹配任何一个字符
        - `*`匹配任意一列零个或多个字符
    - 词组项由`"`包围
- 可用`{`（不包括下界）或`[`（包括下界）、下界、`TO`、上界、`}`（不包括上界）或`]`（包括上界）表示范围查询
- 在项后加上`~`和一个数的话表示进行模糊匹配，即匹配与项的编辑距离（对于单词项）不太远的词或与项中各单词没有相距太远的词组。
- 在项后加上`^`和一个数的话表示项的重要性，这个数越大匹配这项的文档评分越高，不指定视为1。
- 在查询子句后加上`^=`和一个数的话表示把匹配这子句文档的评分设为这个数（不理会其它因素）。
- 用`字段:项`的形式表示要求结果的特定`字段`匹配项，特别地`_val_`字段可用于指定用作评分的函数结果。
- 可以使用布尔操作符组合查询：
    - `AND`或`&&`表示要求结果同时匹配两边的项
    - `NOT`或`!`表示要求结果不匹配紧接的项
    - `OR`或`||`表示要求结果至少匹配两边的项中一个
    - `+`表示要求结果匹配紧接的项
    - `-`表示要求结果不匹配紧接的项
- 可以用`(`和`)`包围子句来形成子查询，以便控制逻辑运算的结合性或字段的作用范围。
- 如果需要查询的词含有`+`、`-`、`&&`、`&#x7c;&#x7c;`、`!`、`(`、`)`、`{`、`}`、`[`、`]`、`^`、`"`、`~`、`*`、`?`、`:`、`/`等有特殊意义的符号，可以在前面加上`\`来转义。
- 查询中可以用`/*`与`*/`包围注释。

参数|用途|默认值
---|---|---
q|查询|必须显式给出
q.op|默认的查询操作符，可以是`AND`、`OR`|`OR`
df|默认字段|
sow|是否用空白把查询切分，然后分别处理各部分|`false`


#### DisMax语法

当使用DisMax查询解析器（`defType`为`dismax`）时，查询语法被大幅简化，只支持`+`、`-`、`"`，其它标准的特殊字符都没有特殊意义，不太可能出现语法错误（奇数个`"`会当作没有词组），适合面向普通人。

参数|用途|默认值
---|---|---
q|查询|必须显式给出
q.alt|默认查询，即查询为空时执行此查询，通常用于匹配全部文档|
qf|用于指定各字段的权重，由一些空格分隔的`字段^重要性`组成|
mm|可选的项中有多少要被匹配，可以是`必须匹配的个数`、`-最多不匹配的个数`、`必须匹配的百分比%`、`-最多不匹配的百分比%`、`项数<有足够多项时的值`（后半部分格式如前，类似有`>`，可用多次）|100%
pf|用于指定各字段的权重（用于提高各单词出现在附近时的评分），由一些空格分隔的`字段^重要性`组成|
ps|词组中各单词最多能移动多远而仍视为匹配|
qs|词组中各单词最多能移动多远而仍视为匹配（用于`qf`中）|
tie|在多个字段都匹配时低分字段与最高分字段的影响力比|0.0
bq|把此查询回到主查询后以影响评分（可指定多次）|
bf|类似于`bq`，只是会把`{!func}`插入到参数


#### 扩展DisMax语法

当使用扩展DisMax查询解析器（`defType`为`edismax`）时，查询语法在DisMax语法的基础上进行了扩展：
- 标准语法中的布尔操作符如`AND`（`&&`）、`OR`（`&#x7c;&#x7c;`）、`NOT`
- 可选择容许`and`、`or`和子查询
- 在出现语法错误时智能转义
- 停用词仍然用于评分，并在全为停用词时保留全部
- 支持纯否定，如`+foo (-foo)`会匹配所有文档

除了DisMax参数外还支持：

参数|用途|默认值
---|---|---
sow|是否用空白把查询切分，然后分别处理各部分|`false`
mm.autoRelax|是否在有子句被从部分字段移除（例如为停用词）后自动放宽必须匹配的子句数|
boost|一个查询列表，结果的分数将会是它对些查询和主查询评分之积|
lowercaseOperators|是否空话把`and`和`or`视为`AND`和`OR`|false
ps|对于`pf`、`pf2`、`pf3`导致的词组查询，单词间的距离
pf2|与`pf`类似，只是对每三个单词形成的词组（2-gram）|
ps2|与`ps`类似，只是用`pf2`|同`ps`
pf3|与`pf`类似，只是对每三个单词形成的词组（3-gram）|
ps3|与ps类似，只是用`pf3`|同`ps`
stopwords|是否去除停用词|
uf|容许显式查询的字段（包括Solr内嵌子查询）列表（用空格分隔，可以用能配符）|`* -_query_`

#### 其它语法

存在许多其它查询解析器用于不同用途，比如：
- 查询父文档、子、可达文档
- 查询类似文档
- 查询某项很少出现的文档
- 要求前缀匹配
- 支持词组内用能配符
- 对结果进行连接
- 对前列结果按另一查询重新评分
- 用XML表达查询

### 高亮

在许多商用的搜索引擎中，结果页中各结果摘要中都会突出显示出现的搜索关键词，以便让用户理解为什么系统会提供这结果。在Solr查询中也可通过在参数`hl.id`中指定需要高亮的字段来做到这效果，例如`curl 'http://localhost:8983/solr/fulltext/select?fl=id,date&hl.fl=title,content&hl=on&q=強积金'`可能得到类似下面的结果：

```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":18,
    "params":{
      "q":"強积金",
      "hl":"on",
      "fl":"id,date",
      "hl.fl":"title,content",
      "_":"1524202118322"}},
  "response":{"numFound":1279,"start":0,"docs":[
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485B!zh-Hant-HK.assist.pdf",
        "date":["2017-09-21T12:13:57Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485E!zh-Hant-HK.assist.pdf",
        "date":["2017-04-22T09:55:46Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485A!zh-Hant-HK.assist.pdf",
        "date":["2017-07-26T05:28:23Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485C!zh-Hant-HK.pdf",
        "date":["2018-01-09T06:43:01Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485!zh-Hant-HK.assist.pdf",
        "date":["2017-09-25T03:10:58Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485G!zh-Hant-HK.pdf",
        "date":["2018-03-16T08:12:23Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485H!zh-Hant-HK.assist.pdf",
        "date":["2017-04-22T10:06:26Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485F!zh-Hant-HK.pdf",
        "date":["2018-03-16T08:10:04Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485I!zh-Hant-HK.assist.pdf",
        "date":["2017-04-22T10:08:21Z"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/cap485D!zh-Hant-HK.assist.pdf",
        "date":["2017-06-26T02:18:31Z"]}]
  },
  "highlighting":{
    "/home/kwong/projects/translate/data/chi/cap485B!zh-Hant-HK.assist.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃(豁免)規例》"],
      "content":["補償基<em>金</em>為<em>強</em>制性公積<em>金</em>計劃所提供的保障內容。\n \n \n \n 4. 現有成員如選擇成為<em>強</em>制性公積<em>金</em>計劃的成員，其在有關職\n業退休註冊計劃下所會享有的權利。\n \n  \n \n \n "]},
    "/home/kwong/projects/translate/data/chi/cap485E!zh-Hant-HK.assist.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃(臨時僱員供款)令》"],
      "content":["經修訂的《<em>強</em>制性公積<em>金</em>計劃 ( 臨時僱員供款 ) 令》的適用情況\n \n \n \n 經本命令修訂的《<em>強</em>制性公積<em>金</em>計劃 ( 臨時僱員供款 ) 令》( 第 485 章，附屬法\n例 E)，就於本命令的生效日期 # 當日或之後開始的供款期而適用。”。\n \n  \n \n \n "]},
    "/home/kwong/projects/translate/data/chi/cap485A!zh-Hant-HK.assist.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃(一般)規例》"],
      "content":["供款附加費的供款率 16-9\n \n 附表 1 計劃資<em>金</em>的投資 S1-1\n \n 附表 2 投資管控合約 S2-1\n \n 附表 3 保管協議的內容 S3-1\n \n 附表 4 罰款 S4-1\n \n 最後更新日期\n \n 26.6.2017\n \n 《<em>強</em>制性公積<em>金</em>計劃 ( 一般 ) 規例》\n \n T-39\n \n 第 485A 章 \n  \n \n   \n 《<em>強</em>制性公積<em>金</em>計劃 ( 一般 ) 規例》\n( 有關《2009 年<em>強</em>制性公積<em>金</em>計劃 ( 修訂 ) 條例》(2009 年第 11 號 ) 的過渡性及保留\n條文，請參看該條例第 24 條。)\n \n "]},
    "/home/kwong/projects/translate/data/chi/cap485C!zh-Hant-HK.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃(費用)規例》"],
      "content":["經修訂的附表 1 第 12 項的適用範圍 1\n \n 附表 1 就《<em>強</em>制性公積<em>金</em>計劃條例》( 第\n485 章 ) 訂明的費用\n \n \n S1-1\n \n 附表 2 就《<em>強</em>制性公積<em>金</em>計劃 ( 一般 ) 規例》\n( 第 485 章，附屬法例 A) 訂明的費\n用\n \n S2-1\n \n 附表 3 就《<em>強</em>制性公積<em>金</em>計劃 ( 豁免 ) 規例》\n( 第 485 章，附屬法例 B) 訂明的費\n用\n \n S3-1\n \n 最後更新日期\n \n 1.1.2018\n \n 《<em>強</em>制性公積<em>金</em>計劃 ( 費用 ) 規例》\n \n T-1\n \n 第 485C 章 \n  \n \n   \n 《<em>強</em>制性公積<em>金</em>計劃 ( 費用 ) 規例》\n \n ( 第 485 章第 46 條 )\n(略去制定語式條文 ——2013年第 1號編輯修訂紀錄 )\n \n  \n \n \n [1999 年 8 月 3 日 ]\n(格式變更 ——2013年第 1號編輯修訂紀錄 )\n \n 1. "]},
    "/home/kwong/projects/translate/data/chi/cap485!zh-Hant-HK.assist.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃條例》"],
      "content":["須備存獲豁免計劃的紀錄冊 1-33\n第 2 部\n \n <em>強</em>制性公積<em>金</em>計劃管理局\n \n 6. <em>強</em>制性公積<em>金</em>計劃管理局的設立 2-1\n \n 6A. "]},
    "/home/kwong/projects/translate/data/chi/cap485G!zh-Hant-HK.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃規則》"],
      "content":["(廢除 ) 9\n \n 最後更新日期\n \n 25.4.2013\n \n 《<em>強</em>制性公積<em>金</em>計劃規則》\n \n T-1\n \n 第 485G 章 \n  \n \n   \n 《<em>強</em>制性公積<em>金</em>計劃規則》\n \n ( 第 485 章第 47 條 )\n(略去制定語式條文 ——2013年第 1號編輯修訂紀錄 )\n \n  \n \n \n [2000 年 12 月 1 日 ]\n(格式變更 ——2013年第 1號編輯修訂紀錄 )\n \n 1. "]},
    "/home/kwong/projects/translate/data/chi/cap485H!zh-Hant-HK.assist.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃(補償申索)規則》"],
      "content":["管理局須向原訟法庭申請指示 3\n \n 最後更新日期\n \n 1.12.2000\n \n 《<em>強</em>制性公積<em>金</em>計劃 ( 補償申索 ) 規則》\n \n T-1\n \n 第 485H 章 \n  \n \n   \n 《<em>強</em>制性公積<em>金</em>計劃 ( 補償申索 ) 規則》\n( 第 485 章第 17(B) 條 )\n \n  \n \n \n [2000 年 12 月 1 日 ]\n \n 1. "]},
    "/home/kwong/projects/translate/data/chi/cap485F!zh-Hant-HK.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃(指明特准限期)公告》"],
      "content":["最後更新日期\n \n 12.11.2015\n \n 《<em>強</em>制性公積<em>金</em>計劃 ( 指明特准限期 ) 公告》\n \n 1 \n第 485F 章 第 1 條\n \n 經核證文本 \n  \n \n   \n 2. "]},
    "/home/kwong/projects/translate/data/chi/cap485I!zh-Hant-HK.assist.pdf":{
      "title":["《<em>強</em>制性公積<em>金</em>計劃(清盤)規則》"],
      "content":["原訟法庭可命令將註冊計劃清盤 3\n \n 最後更新日期\n \n 1.12.2000\n \n 《<em>強</em>制性公積<em>金</em>計劃 ( 清盤 ) 規則》\n \n T-1\n \n 第 485I 章 \n  \n \n   \n 《<em>強</em>制性公積<em>金</em>計劃 ( 清盤 ) 規則》\n( 第 485 章第 34A 條 )\n \n  \n \n \n [2000 年 12 月 1 日 ]\n \n 1. "]},
    "/home/kwong/projects/translate/data/chi/cap485D!zh-Hant-HK.assist.pdf":{
      "title":["《〈<em>強</em>制性公積<em>金</em>計劃(豁免)規例〉(根據第5及16條指明日期)公告》"],
      "content":["為施行《<em>強</em>制性公積<em>金</em>計劃 ( 豁免 ) 規例》第 5 及 16 條，現指\n明 2000 年 5 月 4 日為該兩條所指的指明日期。\n \n 最後更新日期\n \n 3.3.2000\n \n 《〈<em>強</em>制性公積<em>金</em>計劃 ( 豁免 ) 規例〉( 根據第 5 及 16 條指明日期 ) 公告》\n \n 1 \n第 485D 章 第 1 條 \n  \n \n \t 第485D章 \n \t 賦權條文 \n\t 第1條 \n \n \n  "]}}}
```

我们看到现在返回的JSON体中多了一个字段`highlighting`，它是一个映射表，把结果ID映射到另一个映射表，这映射表把字段名映射到高亮片段，高亮片段中关键词被`<em>`和`</em>`包围。

参数|用途|默认值
---|---|---
hl|是否所有高亮|`false`
hl.method|高亮方法：`unified`或`original`或`fastVector`|`original`
hl.fl|需要高亮的字段列表（用逗号或空格分隔），可以用`*`通配|`df`参数值
hl.q|高亮的查询|`q`参数值
hl.qparser|用于`hl.q`的查询解析器|`defType`参数值
hl.requireFieldMatch|只高亮与所查询字段吻合的|false
hl.usePhraseHighlighter|是否只高亮整个词组而非它的组成部分|true
hl.highlightMultiTerm|是否高亮通配查询|true
hl.snippets|每个字段最多的高亮片段数|1
hl.fragsize|高亮内容的预期长度|100
hl.tag.pre|高亮内容开始处插入的记号|`<em>`
hl.tag.post|高亮内容结束处插入的记号|`</em>`
hl.encoder|高度内容编码器，如`html`表示进行HTML转义|
hl.maxAnalyzedChars|为寻找高亮片段所分析的最多字数|51200

以下介绍各高亮方法的区别：
- `unified`是最新的最灵活的，目前已经是官方推荐。它既可以在查询时从`stored`文本寻找高亮，对于`storeOffsetsWithPositions`字段或有项向量的字段还可以用索引时信息加速。
- `original`是Lucene原始的高亮器，可以在底层定制，但不支持断词或断句。它既可以在查询时从`stored`文本寻找高亮，对于有项向量的字段还可以用索引时信息加速，但仍然比其它方法稍慢一点。
- `fastVector`支持多颜色高亮，但对于复杂查询可能不准确。它要求字段有项向量（`termVectors`、`termPositions`、`termOffsets`），这比`storeOffsetsWithPositions`字段占用更多空间。

不同高亮方法有不同的参数以下以`unified`为例：

参数|用途|默认值
---|---|---
hl.offsetSource|位置来源：`ANALYSIS`、`POSTINGS`、`POSTINGS_WITH_TERM_VECTORS`或`TERM_VECTORS`|自动
hl.tag.ellipsis|把多个片段用这字符串连接起来|
hl.defaultSummary|找不到高亮片段时用文本的开首|false
hl.score.k1|BM25频率正规化参数，0表示只与匹配的项数有关|1.2
hl.score.b|BM25长度正规化参数，0表示忽略长度|0.75
hl.score.pivot|`BM25`平均片段长度|87
hl.bs.language|文档划分基于的语言|
hl.bs.country|文档划分基于的地区|
hl.bs.variant|文档划分基于的变种|
hl.bs.type|文档划分方法，用于使整个单位显示在高亮片段中，可以为`SEPARATOR`、`SENTENCE`、`WORD`、`CHARACTER`、`LINE`、`WHOLE`|`SENTENCE`
hl.bs.separator|文本分隔符|

### 分片

在许多电商搜索引擎中，结果页中经常推荐一些分类并提示分类数，以便让用户细化查询。

参数|用途|默认值
---|---|---
facet|是否启用分片|false
facet.query|需要分片的查询|

#### 字段分片

在Solr查询中也可通过在参数`facet.field`中指定需要分片的字段来做到按字段中的项创建分类的效果，例如`curl 'http://localhost:8983/solr/fulltext/select?facet.field=content_type&facet=on&fl=id,date&q=*:*&rows=0'`
可能得到类似下面的结果：

```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":1,
    "params":{
      "q":"*:*",
      "facet.field":"content_type",
      "fl":"id,date",
      "rows":"0",
      "facet":"on",
      "_":"1524202118322"}},
  "response":{"numFound":11234,"start":0,"docs":[]
  },
  "facet_counts":{
    "facet_queries":{},
    "facet_fields":{
      "content_type":[
        "text/html; charset=GB2312",8949,
        "application/pdf",2259,
        "text/html; charset=ISO-8859-1",15]},
    "facet_ranges":{},
    "facet_intervals":{},
    "facet_heatmaps":{}}}
```

我们看到现在返回的JSON体中多了一个字段`facet_counts`，它是一个映射表：
- `facet_fields`字段中给出了各字段中各个项的计数


参数|用途|默认值
---|---|---
facet.field|需分片字段（可多次指定）
facet.prefix|只返回项有指定前缀的片|
facet.contains|只返回项带一个子字符串的片|
facet.contains.ignoreCase|是否在`facet.contains`进行匹配时忽略大小写
facet.matches|只返回项匹配一个正则表达式的片|
facet.sort|片的排序，可以是`count`或`index`|facet.limit正是为`count`否则`index`
facet.limit|最多返回的片数，负表示不限|100
facet.offset|返回的首个片的偏移|0
facet.mincount|只返回至少有这个数量的文档的片|0
facet.missing|是否返回一个由缺失字段的文档组成的片|false
facet.method|分片算法：`enum`（枚举字段的项，适合相异项少的多值字段）、`fc`（把缓存中匹配查询的文档的对应项数向量加起来，适合字段有很多项但每个文档只有很少项时）、`fcs`（对单值字符串字段分块地分片，容许多线程，适合索引快速更新时）|fc（除非是布尔字段且`facet.exists=true`）
facet.enum.cache.minDf|对项启用过滤缓存的最少匹配文档数|0
facet.exists|把非空分片的计数都设为1，从而节省计数的开销，只适用于非前缀树字段（特别地不能用于字符串）|
facet.excludeTerms|要求不返回的项列表|
facet.overrequest.count和facet.overrequest.ratio|在数据分布在多个结点时，只查询每个结点的首facet.overrequest.count+facet.overrequest.ratio*facet.limit个项|10和1.5
facet.threads|最大线程数，0表示用主查询线程|0

#### 决策树分片

有时我们希望按一个字段的分片再对另一个字段分片。例如`curl 'http://localhost:8983/solr/fulltext/select?facet.pivot=content_type,subject&facet=on&q=*:*&rows=0'`可能返回：

```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":1,
    "params":{
      "q":"*:*",
      "facet.pivot":"content_type,subject",
      "rows":"0",
      "facet":"on",
      "_":"1524202118322"}},
  "response":{"numFound":11234,"start":0,"docs":[]
  },
  "facet_counts":{
    "facet_queries":{},
    "facet_fields":{},
    "facet_ranges":{},
    "facet_intervals":{},
    "facet_heatmaps":{},
    "facet_pivot":{
      "content_type,subject":[{
          "field":"content_type",
          "value":"text/html; charset=GB2312",
          "count":8949},
        {
          "field":"content_type",
          "value":"application/pdf",
          "count":2259,
          "pivot":[{
              "field":"subject",
              "value":"legislation",
              "count":2258}]},
        {
          "field":"content_type",
          "value":"text/html; charset=ISO-8859-1",
          "count":15}]}}}
```

参数|用途|默认值
---|---|---
facet.pivot|需要分片的字段列表（用`,`分隔）
facet.pivot.mincount|只列出文档数超过这个数的分类|1

#### 范围分片

在许多电商搜索引擎中，结果页中经常推荐一些按价格范围的分类并提示分类数。在Solr查询中也可以实现按值范围分类的效果，例如`curl 'http://localhost:8983/solr/fulltext/select?facet.range.end=NOW/DAY&facet.range.gap=+3MONTHS&facet.range.other=all&facet.range.start=NOW/DAY-2YEARS&facet.range=date&facet=on&q=*:*&rows=0'`
可能得到类似下面的结果：

```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":6,
    "params":{
      "facet.range":"date",
      "q":"*:*",
      "facet.range.gap":"+3MONTHS",
      "facet.range.other":"all",
      "rows":"0",
      "facet":"on",
      "facet.range.start":"NOW/DAY-2YEARS",
      "_":"1524284370752",
      "facet.range.end":"NOW/DAY"}},
  "response":{"numFound":11234,"start":0,"docs":[]
  },
  "facet_counts":{
    "facet_queries":{},
    "facet_fields":{},
    "facet_ranges":{
      "date":{
        "counts":[
          "2016-04-21T00:00:00Z",0,
          "2016-07-21T00:00:00Z",0,
          "2016-10-21T00:00:00Z",109,
          "2017-01-21T00:00:00Z",1074,
          "2017-04-21T00:00:00Z",447,
          "2017-07-21T00:00:00Z",199,
          "2017-10-21T00:00:00Z",257,
          "2018-01-21T00:00:00Z",173],
        "gap":"+3MONTHS",
        "before":8948,
        "after":0,
        "between":2259,
        "start":"2016-04-21T00:00:00Z",
        "end":"2018-04-21T00:00:00Z"}},
    "facet_intervals":{},
    "facet_heatmaps":{}}}
```

参数|用途|默认值
---|---|---
facet.range|需要分片的字段，可指定多次|
facet.range.start|分片字段项的下界|
f.字段.facet.range.start|特定分片字段项的下界|
facet.range.end|分片字段项的上界|
f.字段.facet.range.end|特定分片字段项的上界|
facet.range.gap|每个范围的长度|
facet.range.hardend|是否让最后一个区间的上界准确地如`facet.range.end`（否则保持最后区间长度为`facet.range.gap`）|false
f.字段.facet.range.hardend|同上，只是只适用于特定字段|
facet.range.include|边界处理方法：`lower`（所有区间包括其下界）、`upper`（所有区间包括其上界）、`edge`（首个区间包括下界，最后一个区间包括其上界）、`outer` （`before`和`after`包括它的界）、`all`，可以指定多次，部分选项可能造成边界处重复计数|`lower`
f.字段.facet.range.include|同上，只是只适用于特定字段|
facet.range.other|额外计数：`none`、`before`（小于首区间下界的）、`after`（大于末区间上界的）、`between`（首区间下界与末区间上界之间的）、`all`，可指定多次|
f.字段.facet.range.other|同上，只是只适用于特定字段|
facet.range.method|分片算法：`filter`（对每个区间分别建立过滤再与主查询结果集求交，可用过滤缓存加速）、`dv`（迭代主查询的结果集，可用过滤缓存或`docValues`加速，不支持日期范围字段和`group.facets`）


#### 区间分片

在范围分片中每个范围区间有相同的长度，但有时我们希望更灵活。例如`curl 'http://localhost:8983/solr/fulltext/select?facet.interval.set=(2000-01-01T00:00:00Z,2010-01-01T00:00:00Z]&facet.interval.set=(2010-01-01T00:00:00Z,*]&facet.interval=date&facet=on&q=*:*&rows=0'`可能得到类似下面的结果：
```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":2,
    "params":{
      "q":"*:*",
      "facet.interval":"date",
      "rows":"0",
      "facet":"on",
      "facet.interval.set":["(2000-01-01T00:00:00Z,2010-01-01T00:00:00Z]",
        "(2010-01-01T00:00:00Z,*]"],
      "_":"1524284370752"}},
  "response":{"numFound":11234,"start":0,"docs":[]
  },
  "facet_counts":{
    "facet_queries":{},
    "facet_fields":{},
    "facet_ranges":{},
    "facet_intervals":{
      "date":{
        "(2000-01-01T00:00:00Z,2010-01-01T00:00:00Z]":8948,
        "(2010-01-01T00:00:00Z,*]":2259}},
    "facet_heatmaps":{}}}
```

参数|用途|默认值
---|---|---
facet.interval|需要分片的字段（可指定多次）
facet.interval.set|区间，由`(`或`[`、下限（没有下限则`*`）、`,`、上限（没有上限则`*`）、`)`或`]`组成
f.字段.facet.interval.set|同上，但只适用于特定字段

### 类似结果

许多电商平台都有“类似产品”，其中一个实现方式为把产品描述中词频类似的当成类似产品。例如`curl 'http://localhost:8983/solr/fulltext/select?df=title&fl=id,title&mlt.fl=content&mlt=true&q=%22%E5%9F%BA%E6%9C%AC%E6%B3%95%22&rows=5&sort=id%20asc'`可能得到类似下面的结果：

```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":17,
    "params":{
      "q":"\"基本法\"",
      "df":"title",
      "mlt":"true",
      "fl":"id,title",
      "mlt.fl":"content",
      "sort":"id asc",
      "rows":"5",
      "_":"1524284370752"}},
  "response":{"numFound":16,"start":0,"docs":[
      {
        "id":"/home/kwong/projects/translate/data/chi/A101!zh-Hant-HK.assist.pdf",
        "title":["中華人民共和國香港特別行政區基本法(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/A102!zh-Hant-HK.assist.pdf",
        "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》附件三所列全國性法律增減的決定(1997年7月1日第八屆全國人民代表大會常務委員會第二十六次會議通過)"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/A103!zh-Hant-HK.assist.pdf",
        "title":["全國人民代表大會常務委員會關於增加《中華人民共和國香港特別行政區基本法》附件三所列全國性法律的決定(1998年11月4日通過)"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/A104!zh-Hant-HK.assist.pdf",
        "title":["全國人民代表大會關於《中華人民共和國香港特別行政區基本法》的決定 (1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
      {
        "id":"/home/kwong/projects/translate/data/chi/A105!zh-Hant-HK.assist.pdf",
        "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》英文本的決定(1990年6月28日通過)"]}]
  },
  "moreLikeThis":{
    "/home/kwong/projects/translate/data/chi/A101!zh-Hant-HK.assist.pdf":{"numFound":11019,"start":0,"docs":[
        {
          "id":"/home/kwong/projects/translate/data/chi/A301!zh-Hant-HK.assist.pdf",
          "title":["中華人民共和國政府和大不列顛及北愛爾蘭聯合王國政府關於香港問題的聯合聲明"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A403!zh-Hant-HK.assist.pdf",
          "title":["1997年全國性法律公布（第2號）"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A1!zh-Hant-HK.assist.pdf",
          "title":["中華人民共和國憲法(1982年12月4日第五屆全國人民代表大會第五次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A203!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會關於批准香港特別行政區基本法起草委員會關於設立全國人民代表大會常務委員會香港特別行政區基本法委員會的建議的決定(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/cap558H!zh-Hant-HK.pdf",
          "title":["《國際組織(特權及豁免權)(海牙國際私法會議亞太區域辦事處)令》"]}]
    },
    "/home/kwong/projects/translate/data/chi/A102!zh-Hant-HK.assist.pdf":{"numFound":11009,"start":0,"docs":[
        {
          "id":"/home/kwong/projects/translate/data/chi/A101!zh-Hant-HK.assist.pdf",
          "title":["中華人民共和國香港特別行政區基本法(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A206!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會常務委員會關於根據《中華人民共和國香港特別行政區基本法》第一百六十條處理香港原有法律的決定(1997年2月23日第八屆全國人民代表大會常務委員會第二十四次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A401!zh-Hant-HK.pdf",
          "title":["《國旗及國徽條例》"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A301!zh-Hant-HK.assist.pdf",
          "title":["中華人民共和國政府和大不列顛及北愛爾蘭聯合王國政府關於香港問題的聯合聲明"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A5!zh-Hant-HK.assist.pdf",
          "title":["中華人民共和國憲法修正案 (2004年3月14日第十屆全國人民代表大會第二次會議通過)"]}]
    },
    "/home/kwong/projects/translate/data/chi/A103!zh-Hant-HK.assist.pdf":{"numFound":10802,"start":0,"docs":[
        {
          "id":"/home/kwong/projects/translate/data/chi/A102!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》附件三所列全國性法律增減的決定(1997年7月1日第八屆全國人民代表大會常務委員會第二十六次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A101!zh-Hant-HK.assist.pdf",
          "title":["中華人民共和國香港特別行政區基本法(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A109!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會常務委員會關於增加《中華人民共和國香港特別行政區基本法》附件三所列全國性法律的決定(2005年10月27日通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A206!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會常務委員會關於根據《中華人民共和國香港特別行政區基本法》第一百六十條處理香港原有法律的決定(1997年2月23日第八屆全國人民代表大會常務委員會第二十四次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A114!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》第十三條第一款和第十九條的解釋(2011年8月26日第十一屆全國人民代表大會常務委員會第二十二次會議通過)"]}]
    },
    "/home/kwong/projects/translate/data/chi/A104!zh-Hant-HK.assist.pdf":{"numFound":11020,"start":0,"docs":[
        {
          "id":"/home/kwong/projects/translate/data/chi/A101!zh-Hant-HK.assist.pdf",
          "title":["中華人民共和國香港特別行政區基本法(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A202!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會關於香港特別行政區第一屆政府和立法會產生辦法的決定(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A203!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會關於批准香港特別行政區基本法起草委員會關於設立全國人民代表大會常務委員會香港特別行政區基本法委員會的建議的決定(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A201!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會關於設立香港特別行政區的決定(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A107!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會常務委員會關於《中華人民共和國香港特別行政區基本法》附件一第七條和附件二第三條的解釋(2004年4月6日第十屆全國人民代表大會常務委員會第八次會議通過)"]}]
    },
    "/home/kwong/projects/translate/data/chi/A105!zh-Hant-HK.assist.pdf":{"numFound":11012,"start":0,"docs":[
        {
          "id":"/home/kwong/projects/translate/data/chi/A101!zh-Hant-HK.assist.pdf",
          "title":["中華人民共和國香港特別行政區基本法(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A206!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會常務委員會關於根據《中華人民共和國香港特別行政區基本法》第一百六十條處理香港原有法律的決定(1997年2月23日第八屆全國人民代表大會常務委員會第二十四次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A601!zh-Hant-HK.pdf",
          "title":["《香港回歸條例》"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A110!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會常務委員會關於批准《中華人民共和國香港特別行政區基本法附件一香港特別行政區行政長官的產生辦法修正案》的決定(2010年8月28日第十一屆全國人民代表大會常務委員會第十六次會議通過)"]},
        {
          "id":"/home/kwong/projects/translate/data/chi/A203!zh-Hant-HK.assist.pdf",
          "title":["全國人民代表大會關於批准香港特別行政區基本法起草委員會關於設立全國人民代表大會常務委員會香港特別行政區基本法委員會的建議的決定(1990年4月4日第七屆全國人民代表大會第三次會議通過)"]}]
    }}}
```

参数|用途|默认值
---|---|---
mlt|是否启用类似结果|`false`
mlt.fl|用于计算相似度的字段|
mlt.count|每个文档返回的类似结果数|5
mlt.mintf|引起注意的最低项频|
mlt.mindf|只考虑出现在至少这么多个文档的词|
mlt.maxdf|只考虑出现在至多这么多个文档的词|
mlt.maxdfpct|只考虑出现在至多这个百分比个文档的词|
mlt.minwl|词引起注意的最小长度|
mlt.maxwl|词引起注意的最大长度|
mlt.maxqt|生成查询包含的最大项数|
mlt.maxntp|每个样本文档最多解析的单词数
mlt.boost|是否按有趣项相关性提升查询
mlt.qf|同DisMax中`qf`参数，但其中字段必须出现在`mlt.fl`

### 数据分析

#### 项频

我们可以统计不同项出现的文档数。例如`curl 'http://localhost:8983/solr/fulltext/select?q=*:*&rows=0&terms.fl=content&terms.stats=true&terms=true'`可能得到类似下面的结果：


```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":52,
    "params":{
      "q":"*:*",
      "terms.stats":"true",
      "terms":"true",
      "terms.fl":"content",
      "rows":"0",
      "_":"1524284370752"}},
  "response":{"numFound":11234,"start":0,"docs":[]
  },
  "terms":{
    "content":[
      "文",11006,
      "例",11003,
      "的",11001,
      "更",10981,
      "新",10978,
      "所",10894,
      "及",10873,
      "6",10872,
      "定",10817,
      "有",10793]},
  "indexstats":{
    "numDocs":11234}}
```

参数|用途|默认值
---|---|---
terms|是否启用项组件|`false`
terms.fl|用于获取项的字段，可多次指定|
terms.list|要取得文档数的项列表（用`,`分隔）
terms.limit|返回的最大项数，负表示不限，不提供这参数则要提供`terms.upper`|
terms.lower|停止于哪个项，前面的项不再返回|
terms.lower.incl|是否包括上述的停止项|`true`
terms.mincount|返回项对应的最少文档数|
terms.maxcount|返回项对应的最大文档数，负表示不限|-1
terms.prefix|返回项的前缀|
terms.raw|是否返回原始字符（即使对人不可读）|
terms.regex|项满足的正则表达式
terms.regex.flag|正则表达式模式：`case_insensitive`、`comments`、`multiline`、`literal`、`dotall`、`unicode_case`、`canon_eq`、`unix_lines`|
terms.stats|是否返回索引统计
terms.sort|排序依据：`count`或`index`
terms.ttf|是否返回总项频|
terms.upper|停止于哪个项，后面的项不再返回|
terms.upper.incl|是否包括上述的停止项|false

#### 描述性统计量

我们可以取得关于字段项的描述性统计量。例如`curl 'http://localhost:8983/solr/fulltext/select?q=*:*&rows=0&stats.field=date&stats=true'`可能得到类似下面的结果：


```json
{
  "responseHeader":{
    "zkConnected":true,
    "status":0,
    "QTime":3,
    "params":{
      "q":"*:*",
      "stats":"true",
      "rows":"0",
      "_":"1524284370752",
      "stats.field":"date"}},
  "response":{"numFound":11234,"start":0,"docs":[]
  },
  "stats":{
    "stats_fields":{
      "date":{
        "min":"2001-09-18T15:35:44Z",
        "max":"2018-03-28T04:30:11Z",
        "count":11207,
        "missing":27,
        "sum":1.3889295079897E16,
        "mean":"2009-04-10T05:24:03.981Z",
        "sumOfSquares":1.740206144832493E28,
        "stddev":1.2969299676900275E11}}}}
```

参数|用途|默认值
---|---|---
stats|是否返回统计信息|`false`
stats.field|需统计的字段，可多次指定|

#### 更深入的处理

在`solrconfig.xml`中启用`analytics`：

```xml
<searchComponent name="analytics" class="org.apache.solr.handler.component.AnalyticsComponent" />
<requestHandler name="/select" class="solr.SearchHandler">
<arr name="last_components">
<str>analytics</str>
</arr>
</requestHandler>
<requestHandler name="/analytics" class="org.apache.solr.handler.AnalyticsHandler" />
```

然后就可以把分析请求通过`analytics`参数传入：

```
curl --data-urlencode '
analytics={
  "functions": {
    "sale()": "mult(price,quantity)"},
  "expressions" : {
    "max_sale" : "max(sale())",
    "med_sale" : "median(sale())"},
  "groupings" : {
    "sales" : {
      "expressions" : {
        "stddev_sale" : "stddev(sale())",
        "min_price" : "min(price)",
        "max_quantity" : "max(quantity)"},
      "facets" : {
        "category" : {
          "type" : "value",
          "expression" : "fill_missing(category, 'No Category')",
          "sort" : {
          "criteria" : [
            {
              "type" : "expression",
              "expression" : "min_price",
              "direction" : "ascending"},
            {
              "type" : "facetvalue",
              "direction" : "descending"}],
          "limit" : 10}},
      "temps" : {
        "type" : "query",
        "queries" : {
          "hot" : "temp:[90 TO *]",
          "cold" : "temp:[* TO 50]"}}}}}}'
http://localhost:8983/solr/sales/select?q=*:*&wt=json&rows=0
```

更强大的分析可能需要用到流语言，它可以批处理式地高效完成各种任务，例如：

```
curl --data-urlencode 'expr=search(enron_emails,
q="from:1800flowers*",
fl="from, to",
sort="from asc",
qt="/export")' http://localhost:8983/solr/enron_emails/stream
```

可能导致返回：

```
{"result-set":{"docs":[
{"from":"1800flowers.133139412@s2u2.com","to":"lcampbel@enron.com"},
{"from":"1800flowers.93690065@s2u2.com","to":"jtholt@ect.enron.com"},
{"from":"1800flowers.96749439@s2u2.com","to":"alewis@enron.com"},
{"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
{"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
{"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
{"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
{"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
{"from":"1800flowers@shop2u.com","to":"ebass@enron.com"},
{"from":"1800flowers@shop2u.com","to":"lcampbel@enron.com"},
{"from":"1800flowers@shop2u.com","to":"lcampbel@enron.com"},
{"from":"1800flowers@shop2u.com","to":"lcampbel@enron.com"},
{"from":"1800flowers@shop2u.com","to":"ebass@enron.com"},
{"from":"1800flowers@shop2u.com","to":"ebass@enron.com"},
{"EOF":true,"RESPONSE_TIME":33}]}
}
```
