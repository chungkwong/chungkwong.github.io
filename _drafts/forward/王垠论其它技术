　　
====== 一种新的操作系统设计 ======


我一直在试图利用程序语言的设计原理，设计一种超越“Unix 哲学”的操作系统。这里是我的设想：

    * 这种系统里面的程序间通信不使用无结构的字符串，而是使用带有类型和结构的数据。在这样的系统里面，Unix 和其它类似操作系统（比如 Windows）里的所谓“应用程序”的概念基本上完全消失。系统由一个个很小的“函数”组成，每个函数都可以调用另外一个函数，通过参数传递数据。每个函数都可以手动或者自动并发执行。用现在的系统术语打个比方，这就像是所有代码都是“库”代码，而不存在独立的“可执行文件”。
    * 由于参数是数据结构而不是字符串，这避免了程序间通信繁琐的编码和解码过程。使得“进程间通信”变得轻而易举。任何函数都可以调用另一个函数来处理特定类型的数据，这使得像 “OLE 嵌入”这样的机制变得极其简单。
    * 所有函数由同一种先进的高级程序语言写成，所以函数间的调用完全不需要“翻译”。不存在 SQL injection 之类由于把程序当成字符串而产生的错误。
    * 由于这种语言不允许应用程序使用“指针运算”，应用程序不可能产生 segfault 一类的错误。为了防止不良用户手动在机器码里面加入指针运算，系统的执行的代码不是完全的机器代码，而必须通过进一步的验证和转换之后才会被硬件执行。这有点像 JVM，但它直接运行在硬件之上，所以必须有一些 JVM 没有的功能，比如把内存里的数据结构自动换出到硬盘上，需要的时候再换进内存。
    * 由于没有指针运算，系统可以直接使用“实地址”模式进行内存管理，从而不再需要现代处理器提供的内存映射机制以及 TLB。内存的管理粒度是数据结构，而不是页面。这使得内存访问和管理效率大幅提高，而且简化了处理器的设计。据 Kent Dybvig 的经验，这样的系统的内存使用效率要比 Unix 类的系统高一个数量级。
    * 系统使用与应用程序相同的高级语言写成，至于“系统调用”，不过是调用另外一个函数。由于只有这些“系统驱动函数”才有对设备的“引用”，又因为系统没有指针运算，所以用户函数不可能绕过系统函数而非法访问硬件。
    * 系统没有 Unix 式的“命令行”，它的“shell”其实就是这种高级语言的 REPL。用户可以在终端用可视化的结构编辑方式输入各种函数调用，从而启动进程的运行。所以你不需要像 Unix 一样另外设计一种毛病语言来“粘接”应用程序。
    * 所有的数据都作为“结构”，保存在一个分布式的数据共享空间。同样的那个系统语言可以被轻松地发送到远程机器，调用远程机器上的库代码，执行任意复杂的查询索引等动作，取回结果。这种方式可以高效的完成数据库的功能，然而却比数据库简单很多。所谓的“查询语言”（比如 SQL，Datalog，Gremlin，Cypher）其实是多此一举，它们远远不如普通的程序语言强大。说是可以让用户“不需要编程，只提出问题”，然而它们所谓的“优化”是非常局限甚至不可能实现的，带来的麻烦远比直接编程还要多。逻辑式编程语言（比如 Prolog）其实跟 SQL 是一样的问题，一旦遇到复杂点的查询就效率低下。所以系统不使用关系式数据库，不需要 SQL，不需要 NoSQL，不需要 Datalog。
    * 由于数据全都是结构化的，所以没有普通操作系统的无结构“文件系统”。数据结构可能通过路径来访问，然而路径不是一个字符串或者字符串模式。系统不使用正则表达式，而是一种类似 NFA 的数据结构，对它们的拆分和组合操作不会出现像字符串那样的问题，比如把 ''/a/b/'' 和 ''/c/d'' 串接在一起就变成错误的 ''/a/b/'' '' /c/d''。
    * 所有的数据在合适的时候被自动同步到磁盘，并且进行容错处理，所以即使在机器掉电的情况，绝大部分的数据和进程能够在电源恢复后继续运行。
    * 程序员和用户几乎完全不需要知道“数据库”或者“文件系统”的存在。程序假设自己拥有无穷大的空间，可以任意的构造数据。根据硬件的能力，一些手动的存盘操作也可能是有必要的。
    * 为了减少数据的移动，系统或者用户可以根据数据的位置，选择： 1）迁移数据，或者 2）迁移处理数据的“进程”。程序员不需要使用 MapReduce，Hadoop 等就能进行大规模并行计算，然而表达能力却比它们强大很多，因为它们全都使用同一种程序语言写成。


我曾经以为我是第一个想到这个做法的人。可是调查之后发现，很多人早就已经做出了类似的系统。Lisp Machine 似乎是其中最接近的一个。Oberon 是另外一个。IBM System/38 是类似系统里面最老的一个。最近一些年出现的还有微软的 Singularity，另外还有人试图把 JVM 和 Erlang VM 直接放到硬件上执行。

所以这篇文章的标题其实是错的，这不是一种“新的操作系统设计”。它看起来是新的，只不过因为我们现在用的操作系统忘记了它们本该是什么样子。我也不该说它“超越了 Unix 哲学”，而应该说，所谓的 Unix 哲学其实是历史的倒退。

====== 十年前的我的来信 ======


作者：王垠


　　===== 十年前的我的来信 =====


今天收到一封 email，反对我在《一种新的操作系统设计》一文中提到的所有想法。让我想起一句话，每一个好的想法都要经受疯狂的反对。很感谢这位读者。他让我想起十年前的那个我。如果十年前那个我来到今天看到这篇文章，恐怕也会发这样的信给现在的我吧。见到年轻的我真好，所以把这封 email 记录在这里。它会鼓舞着我，它让我明白，我能做到很多人想象不到的事情，看到他们看不到的东西。

简单的概述

首先

看了一篇你的<一种新的操作系统设计>的文章!有一些简单的想法!希望可以与你交流一下! 只限于技术性探讨,没有其他意思和目的!言辞不当之处请谅解!同时水平有限有不当之处可以批评!(另外我看了你的Ydiff,如果以后有机会也可以聊聊)

文章是Markdown的,复制到邮件中或许会变形,如果看着不方便可以把原文档发给你!

我不理解你是如何理解"操作系统"这个概念的,也不明白你所谓的超越“Unix 哲学”是什么含义(但是你既然提到了Unix我就暂且认为你想要要的是运行于硬件的操作系统)。 但是我觉得你关于操作系统的理解完全是基于用户层面的.可以看出你是个理想主义者,希望系统按照自己想要的方式做事情!

你所说的11条操作系统的设想,是完全不靠谱的.有的甚至背离的"经典操作系统"的概念的,或者你设想的根本不能算一个操作系统而是架设在操作系统上的一层壳!(就像Hadoop一样是一个分布式'组织系统'),只不过Hadoop只解决了分布式计算(MapReduce)和分布式存储(HDFS),而你想要的更多,但也只是停留在操控层面的,你只是希望屏蔽一些底层的东西,让业务层面的变得更简单,或者看起来更简单(事实上不是每个人的思维方式都和你一样的,有些人认为面向对象的思维方式更自然,而有的人不是,一个完美主义者或者理想主义者总是试图找到一条普世的理论去阐释这个世界,于是有了"弦论",在科学界一是一,二是二的二元论断是非常值得推崇的,但是面对人性,面对需求你或许应该思考更多,爱因斯坦说上帝不玩骰子,然而他最终还是接受(至少是默许)了量子论,bash已经很好用了,却还有人鼓捣出各种各样的shell.)!

关于你文章对操作系统理解的总结:

通信是结构化的 一切都是用"超语言"定义的函数,所有的函数的规范是一元的 一切(系统本身,"函数程序",用户Shell程序)都用超语言实现,超语言不能有指针运算 用"对象数据库"取代关系数据库 不需要文件系统 用户透明与"无限" 为了避免数据移动产生的IO开销,优先自动迁移进程! 标题不好说

你的设计思路对操作系统没有任何现实意义,更多的是空中楼阁,水中月!是没有任何现实基础的"美好瞎想"!

你的文章中只有推翻,而没有建设!你只是说这个不要,那个不要(不要进程,不要文件系统,不要关系数据库...),而没有提供实际的可以取代他们的真正有意义的方案!

你的思维方式不是在设计一种操作系统,而是在设计一种编程语言!

按照你的理论:Python或者Java就是一个完美的操作系统!它几乎满足你要的一切!

超语言(上帝语言)的概念很吸引人,但这绝对是错误的!正如我上面所说人性和需求是复杂的,是动态的,是多变的,哪怕你提供是诸如"英语"一样的通用语言也会有很多人需要添加一个"中文翻译器"!完全不用翻译的意思就是你得要求所有人都遵循和接受你的思维方式!当然或许你会说这种超语言满足一切人的需求,拥有一切易用的特性!好吧,如果真的有上帝语言可以跟任何人沟通,我想还是会有很多人愿意尝试的!

你完全没有理解操作系统是什么,他扮演什么角色具体要处理哪些工作!你所要求的都只是操作系统的外延,而不是操作系统的内涵!你博客的文章标题或许变成<我需要这样一套分布式管理系统>或者<一种新编程语言的设计>更加合适!

就现在计算机的体系结构而言没有指针运算是不现实的,就架构而言现实机器大多是基于寄存器的,讨论基于堆栈的虚拟机对于设计和实现操作系统没有意义,事实上无论那种机型,想要快速随机的访问特定位置的数据间接寻址是一个非常不错的工具,除非你完全抛弃"间接寻址"!或许你在"语法解析"这种逻辑层面不需要这种特性,但系统层面的间接寻址是必须的,除非你设计一套新基于更高理论水平和哲学层次的硬件!

把系统看作为一个"仆人",只要对他下命令他就能按照你想要的方式为你做任何事情(这或许是你的终极目标或者是你目标的一种更为极端的表述)!只想用高级语言的特性而忽略计算机体系结构设想出来的东西根本就不能算作"典型意义上的操作系统"!按照你的方式我完全可以这样设计一个操作系统:只要我告诉他我想做什么,计算机就可以为我做任何事情,比如去厨房做个番茄炒蛋!他就傻乎乎的跑去买番茄和鸡蛋然后...这样的设计岂不是更完美?

我觉得你可能是做"语法分析"的!或许对各种语言的"哲学思想"有独到的见解,但是把这种思想或者更高层次的抽象带到"操作系统"的设计上就是很大职业病!为此你需要推翻现代计算机的体系结构,重新设计和研究可计算理论,不切实际的泛泛而论只会让我看到你的幼稚与任性,露脸与现眼只差一步!

如果你希望你做的是惊世骇俗,前五百年后五百年都不曾有和超越的东西.请先不要将它叫做"操作系统",或者跟"Unix哲学"比较!如果必须这样做,请先搞搞清楚什么事操作系统和Unix哲学!

我的一些相对现实与丑陋(没有那么理想化,很多甚至是迂腐)的看法

我理解的操作系统

最原始的需求:如果将计算机看作一堆冷冰冰的电子元件操作系统的角色就是"驱动,它将这堆冷冰冰的器件合理的组织起来,让他们构成一个系统团结起来工作;要知道程序是直接可以跑在机器上的,不需要任何系统的,但是这意味着你就必须面对这一堆冷冰冰的电子元件了! 我们要的更多:尽管硬件系统变越来越复杂,如果将计算机的计算能力,内存空间,外存空间等看作资源,然而这部分资源终究还是有限的;操作系统的目标是如何更有效的使用和管理这些资源!就因为资源有限所以才需要管理,才需要操作系统,当然你所设想的"无限空间",不需要"文件系统"这种概念完全是背离的! 基于以上两点,现在多数真正的操作系统是面向"硬件"的,如何高效的利用硬件是他们的重点!(宏内核与微内核的争论在这一点上看起来像是对立的,事实上确实是这样么?我觉得,孜孜不倦的挖掘计算机的潜力是每个系统开发者的目标与乐趣所在)

程序,进程与函数

什么是程序什么是进程?去掉进程的概念是不是意味着你需要用别的方法代替?函数吗?事实上他们完全不是一回事儿,进程概念的存在是为了让程序看起来是并行执行的(还是以上这一点,为了更有效的利用CPU资源)!

函数呢? 函数是一种"可计算模型"的表示方法(你完全可以用图灵机,lambda演算,寄存器模型,或者递归来代替,因为他们的本质是一样的)!

你只是用函数描述了计算方法,而进程是有时间考量的,可计算理论中并没有"时间"这个维度!你抛弃了进程,就势必要在函数的定义一个时间维度,在你的观念里函数是一切,它可以做任何事情!现实意义上的计算机更多的是基于寄存器模型和递归模型的,编译器大多数只是将函数表述翻译成寄存器表述或者递归表述,这个过程是一一对应的,他们都是静态的,编译器在这个过程无法给他附上时间维度.编译器不做,操作系统也不做,也就意味着程序员得在代码中加入控制调度的代码,好吧我们回到了最原始的时代了.(当然除非你假设CPU的计算能力是无限的,任何函数都是可以并行执行的,函数的执行是不需要时间的,函数之间的依赖关系是不存在的...就另当别论了)

"“系统调用”，只不过是调用另外一个函数,"对于你的轻蔑傲慢的语气我真的不想吐槽!事实上系统调用还真是调用另外一个函数,对于很多微内核系统虽然其实现是"基于消息"的,但是其外在依然表现为"过程调用"形式,对于被调用资源的实现也依然是个函数! 我不知道你是如何理解函数这个概念的,"任何可计算的问题都能用可计算模型(比如函数)来表示,而可计算模型之外的则是不可计算的",这或许看起来像句废话,但他确实证明你所说的,可计算的一切都可以是函数,但是仅此而已,没有任何外延了,如何安排计算,如何调度计算,可计算理论并没有给出任何有意义的说明;

当然实际问题要复杂的多,除了时间问题,还有你提到的权限问题等!

关于关系数据库与文件系统

系统不需要 SQL，不需要关系式数据库。

我需要强调一点:关系数据库并不仅仅是一个数据存储方案,也是一个数据"计算方案"(很多人用他是不是因为存储方便,事实上它存储不方便也限制多多,诸如数据必须是结构化的,对于树形的或者图形结构的数据还需要扁平化)! 数据存储的方案有很多,你可以直接将数据按一定的格式排版放到一个文件,可以使用键值模型存储简单的数据,或者将数据保存为特定通用格式(比如JSON,XML),或者有很多基于树形结构的文档数据库.基于图论的图形数据库(Neo4j)和狗屎的对象数据,当然也包括基于关系模型的关系数据库!(或许有一天有人会来个大一统,但是谁取代谁的说法显然是不靠谱的),他们各有各的优缺点,而关系数据库更是基于严谨成熟的关系数据理论构建的,他的数据结构简单,运算简介高效,不明白为什么有人会讨厌SQL,事实上他是最简洁方便直观的工具,他隐藏了很多计算细节,甚至你只需要告诉他我需要什么(而不是如何做),他就能很好的为你提供服务!

如果一切都是对象,随意的构造数据,存取无疑是最方便的),但构建在对象之上的算法就必须自己实现(这将会是灾难),相比于自己在对象之上实现算法,"构造结构化数据"显然更简单(或者机械化),基于成熟的关系数据理论也可以让你避免很多错误!我想这也是关系数据库占统治地位的主要原因!

对于文件系统,理解有很多种,基于磁盘的文件系统和构建于文件系统之上的"文件系统"!操作系统意义上的是前者,而后者大多数是是一些"虚拟文件系统",网络文件系统,或者分布式文件系统!后者更接近于"文件管理系统",他没有解决数据如何存储在物理介质上的问题,主要面向业务,提供"文件系统透明"这一概念!操作系统设计提出"不要文件系统"和"金三胖不要姑父"一样荒唐!

解码是必须的

解码(解析)是一个"理解数据"的过程!

你可以把任何数据看作一个"有特定语意的对象",作为人很容易凭直觉和经验认定12345是数字,abcde是字母!但对于计算机他只是一块放在特定位置的"没有特定意义"的"数据",我们使用数据就必须对其赋予一定的含义,知道他是什么,如何组织!计算机不会下意识的知道12345是数字,而abcde是字母!理解数据必须存在,哪怕是一个结构化的数据,要使用它你就必须理解他,同时要让计算机理解他!(事实上你在判断12345是数字之前已经对他做了解码了)

所以我的结论是:解码是必须的!不是所有事情都是理所应当的, 或许向你这样的人思维速度非常快,有些问题你凭直觉和经验就得出了结论!以至于你甚至意识不到你对问题做出了思考!但是理性是必然存在的!

那么这个问题就变成了"谁来解码"!

目前而言可以有下面几个选择:

计算机自己(硬件系统和操作系统)

事实上计算机有一定的解析数据的能力 就冯诺依曼体系结构的计算机而言,它要求程序按照特定的指令系统被编码存放,这种规范的是一个有限的集合!无限延伸让他处理任何的数据是不可能的(比如你给计算机一个Torrent文件他或许只是个文本,但让计算机知道接下来要做什么是不可能的,不可能任何系统都安装了这个解码器,因为不是所有人都需要的,你必须许更具特定的需求自己安装Bencode/Bdecode)!更高层次的解析(理解)需要具体问题具体处理,除非你颠覆现有的体系结构重新设计基于更高理论水平和哲学层次的可计算模型. 把所有的东西交给系统来处是不切实际且没有必要的(如果特定场景或许可以),所以你或许应该把这些事情交给下面的选项来做!

编译器(或者解释器) 当我们编译一个C的结构体的时候,编译器提供了编码和解码,它根据语法定义的数据的宽度得到特定偏移,将数据按照特定偏移排布在特定的位置这是一个编码的过程,计算机根据自己的指令系统的定义和规范对编码的数据进行加载和运算,这是一个解码的过程!从函数式编译到寄存器模型亦是一个翻译的过程,这个翻译过程为了让不同系统之间相互交流编码和解码是不可避免的!

程序库

我们很容易的使用已有的库去解析JSON,XML,让他们变成你所谓的对象,应为你或许觉得在用户层面上使用对象的概念会更简单!

用户(程序员): 对前无古人的个性化数据赋予一定的语意,用户必须制定特定的格式或者协议,然后手动编写相应的代码实现编码与解析!然而这是大多数创造力的来源!如果你做创造性的工作,我想这会是大多数情况!

关于如何解决分布式问题

你提到的移动"计算"优于"移动数据",这在大多数情况下是正确,包括Hadoop这种系统也是那么做的,这并无新意!

然而事实上分布式问题的核心在于调度,在于如何抽象成一个分布式计算模型来一劳永逸的解决任何可分布式计算的问题,在这里还要明确一点有很多问题是不可分布式计算的,就是因为存在这种问题,构建一劳永逸的模型就几乎成为不可能的至少是有代价的事情,MapReduce或许是一个很不错的模型,但在特定的情景下依然会显得力不从心,同时无可避免的引入了无关代码去分解问题(只是将这个过程模式化了),很多情况下特定的业务逻辑下,这种方案并不是最高效的.

而Java和其他一些分布式方案并不是真正意义上的"分布式模型",他并没有解决任何"问题分解"和"调度"的问题,程序员仍然需要根据自己的业务需求分解简化问题,决定哪些问题在哪些节点运行,何时运行!

不需要显式的使用特定的方法就能让系统跟安排进程执行一样自动的分布这些计算和数据到不同的节点的通用系统我没见过(或许还真有)!

====== unix的缺陷 ======


作者：王垠

我想通过这篇文章解释一下我对 Unix 哲学本质的理解。我虽然指出 Unix 的一个设计问题，但目的并不是打击人们对 Unix 的兴趣。虽然 Unix 在基础概念上有一个挺严重的问题，但是经过多年的发展之后，这个问题恐怕已经被各种别的因素所弥补（比如大量的人力）。但是如果开始正视这个问题，我们也许就可以缓慢的改善系统的结构，从而使得它用起来更加高效，方便和安全，那又未尝不可。同时也希望这里对 Unix 命令本质的阐述能帮助人迅速的掌握 Unix，灵活的应用它的潜力，避免它的缺点。

通常所说的“Unix哲学”包括以下三条原则[Mcllroy]：

      - 一个程序只做一件事情，并且把它做好。
      - 程序之间能够协同工作。
      - 程序处理文本流，因为它是一个通用的接口。



这三条原则当中，前两条其实早于 Unix 就已经存在，它们描述的其实是程序设计最基本的原则——模块化原则。任何一个具有函数和调用的程序语言都具有这两条原则。简言之，第一条针对函数，第二条针对调用。所谓“程序”，其实是一个叫 "main" 的函数（详见下文）。

所以只有第三条（用文本流做接口）是 Unix 所特有的。下文的“Unix哲学”如果不加修饰，就特指这第三条原则。但是许多的事实已经显示出，这第三条原则其实包含了实质性的错误。它不但一直在给我们制造无需有的问题，并且在很大程度上破坏前两条原则的实施。然而，这条原则却被很多人奉为神圣。许多程序员在他们自己的程序和协议里大量的使用文本流来表示数据，引发了各种头痛的问题，却对此视而不见。

Linux 有它优于 Unix 的革新之处，但是我们必须看到，它其实还是继承了 Unix 的这条哲学。Linux 系统的命令行，配置文件，各种工具之间都通过非标准化的文本流传递数据。这造成了信息格式的不一致和程序间协作的困难。然而，我这样说并不等于 Windows 或者 Mac 就做得好很多，虽然它们对此有所改进。实际上，几乎所有常见的操作系统都受到 Unix 哲学潜移默化的影响，以至于它们身上或多或少都存在它的阴影。

Unix 哲学的影响是多方面的。从命令行到程序语言，到数据库，Web…… 计算机和网络系统的方方面面无不显示出它的影子。在这里，我会把众多的问题与它们的根源——Unix哲学相关联。现在我就从最简单的命令行开始吧，希望你能从这些最简单例子里看到 Unix 执行命令的过程，以及其中存在的问题。（文本流的实质就是字符串，所以在下文里这两个名词通用。）


===== 一个 Linux 命令运行的基本过程 =====



几乎每个 Linux 用户都为它的命令行困惑过。很多人（包括我在内）用了好几年 Linux 也没有完全的掌握命令行的用法。虽然看文档看书以为都看透了，到时候还是会出现莫名其妙的问题，有时甚至会耗费大半天的时间在上面。其实如果看透了命令行的本质，你就会发现很多问题其实不是用户的错。Linux 遗传了 Unix 的“哲学”，用文本流来表示数据和参数，才导致了命令行难学难用。

我们首先来分析一下 Linux 命令行的工作原理吧。下图是一个很简单的 Linux 命令运行的过程。当然这不是全过程,但是更具体的细节跟我现在要说的主题无关。


从上图我们可以看到，在 ls 命令运行的整个过程中，发生了如下的事情：

    - shell（在这个例子里是bash）从终端得到输入的字符串 "ls -l *.c"。然后 shell 以空白字符为界，切分这个字符串，得到 "ls", "-l" 和 "*.c" 三个字符串。

    - shell 发现第二个字符串是通配符 "*.c"，于是在当前目录下寻找与这个通配符匹配的文件。它找到两个文件： foo.c 和 bar.c。

    - shell 把这两个文件的名字和其余的字符串一起做成一个字符串数组 {"ls", "-l", "bar.c", "foo.c"}. 它的长度是 4.

    - shell 生成一个新的进程，在里面执行一个名叫 "ls" 的程序，并且把字符串数组 {"ls", "-l", "bar.c", "foo.c"}和它的长度4，作为ls的main函数的参数。main函数是C语言程序的“入口”，这个你可能已经知道。

    - ls 程序启动并且得到的这两个参数（argv，argc）后，对它们做一些分析，提取其中的有用信息。比如 ls 发现字符串数组 argv 的第二个元素 "-l" 以 "-" 开头，就知道那是一个选项——用户想列出文件详细的信息，于是它设置一个布尔变量表示这个信息，以便以后决定输出文件信息的格式。

    - ls 列出 foo.c 和 bar.c 两个文件的“长格式”信息之后退出。以整数0作为返回值。

    - shell 得知 ls 已经退出，返回值是 0。在 shell 看来，0 表示成功，而其它值（不管正数负数）都表示失败。于是 shell 知道 ls 运行成功了。由于没有别的命令需要运行，shell 向屏幕打印出提示符，开始等待新的终端输入……



从上面的命令运行的过程中，我们可以看到文本流（字符串）在命令行中的普遍存在：

      * 普通列表项目用户在终端输入是字符串。

      * shell 从终端得到的是字符串，分解之后得到 3 个字符串，展开通配符后得到 4 个字符串。

      * ls 程序从参数得到那 4 个字符串，看到字符串 "-l" 的时候，就决定使用长格式进行输出。



接下来你会看到这样的做法引起的问题。


===== 冰山一角 =====



在《Unix 痛恨者手册》(The Unix-Hater's Handbook, 以下简称 UHH)这本书开头，作者列举了 Unix 命令行用户界面的一系列罪状，咋一看还以为是脾气不好的初学者在谩骂。可是仔细看看，你会发现虽然态度不好，他们某些人的话里面有非常深刻的道理。我们总是可以从骂我们的人身上学到一些东西，所以仔细看了一下，发现其实这些命令行问题的根源就是“Unix 哲学”——用文本流（字符串）来表示参数和数据。很多人都没有意识到，文本流的过度使用，引发了太多问题。我会在后面列出这些问题，不过我现在先举一些最简单的例子来解释一下这个问题的本质，你现在就可以自己动手试一下。

    - 在你的 Linux 终端里执行如下命令（依次输入：大于号，减号，小写字母l）。这会在目录下建立一个叫 "-l" 的文件。

        $ >-l

    - 执行命令 ls * （你的意图是以短格式列出目录下的所有文件）。



你看到什么了呢？你没有给 ls 任何选项，文件却出人意料的以“长格式”列了出来，而这个列表里面却没有你刚刚建立的那个名叫 "-l" 的文件。比如我得到如下输出：

    -rw-r--r-- 1 wy wy 0 2011-05-22 23:03 bar.c

    -rw-r--r-- 1 wy wy 0 2011-05-22 23:03 foo.c



到底发生了什么呢？重温一下上面的示意图吧，特别注意第二步。原来 shell 在调用 ls 之前，把通配符 * 展开成了目录下的所有文件，那就是 "foo.c", "bar.c", 和一个名叫 "-l" 的文件。它把这 3 个字符串加上 ls 自己的名字，放进一个字符串数组 {"ls", "bar.c", "foo.c", "-l"}，交给 ls。接下来发生的是，ls 拿到这个字符串数组，发现里面有个字符串是 "-l"，就以为那是一个选项：用户想用“长格式”输出文件信息。因为 "-l" 被认为是选项，就没有被列出来。于是我就得到上面的结果：长格式，还少了一个文件！

这说明了什么问题呢？是用户的错吗？高手们也许会笑，怎么有人会这么傻，在目录里建立一个叫 "-l" 的文件。但是就是这样的态度，导致了我们对错误视而不见，甚至让它发扬光大。其实撇除心里的优越感，从理性的观点看一看，我们就发现这一切都是系统设计的问题，而不是用户的错误。如果用户要上法庭状告 Linux，他可以这样写：

起诉状

原告：用户 luser

被告：Linux 操作系统

事由：合同纠纷

    - 被告的文件系统给用户提供了机制建立这样一个叫 "-l" 的文件，这表示原告有权使用这个文件名。

    - 既然 "-l" 是一个合法的文件名，而 "*" 通配符表示匹配“任何文件”，那么在原告使用 "ls *" 命令的时候，被告就应该像原告所期望的那样，以正常的方式列出目录下所有的文件，包括 "-l" 在内。

    - 但是实际上原告没有达到他认为理所当然的结果。"-l" 被 ls 命令认为是一个命令行选项，而不是一个文件。

    - 原告认为自己的合法权益受到侵犯。



我觉得为了免去责任，一个系统必须提供切实的保障措施，而不只是口头上的约定来要求用户“小心”。就像如果你在街上挖个大洞施工，必须放上路障和警示灯。你不能只插一面小旗子在那里，用一行小字写着: “前方施工，后果自负。”我想每一个正常人都会判定是施工者的错误。

可是 Unix 对于它的用户却一直是像这样的施工者，它要求用户：“仔细看 man page，否则后果自负。”其实不是用户想偷懒，而是这些条款太多，根本没有人能记得住。而且没被咬过之前，谁会去看那些偏僻的内容啊。但是一被咬，就后悔都来不及。完成一个简单的任务都需要知道这么多可能的陷阱，那更加复杂的任务可怎么办。其实 Unix 的这些小问题累加起来，不知道让人耗费了多少宝贵的时间。

如果你想更加确信这个问题的危险性，可以试试如下的做法。在这之前，请新建一个测试用的目录，以免丢失你的文件！ 

    - 在新目录里，我们首先建立两个文件夹 dir-a, dir-b 和三个普通文件 file1，file2 和 "-rf"。然后我们运行 "rm *"，意图是删除所有普通文件，而不删掉目录。

    $ mkdir dir-a dir-b

    $ touch file1 file2

    $ > -rf

    $ rm *

    - 然后用 ls 查看目录。



你会发现最后只剩下一个文件: "-rf"。本来 "rm *" 只能删除普通文件，现在由于目录里存在一个叫 "-rf" 的文件。rm 以为那是叫它进行强制递归删除的选项，所以它把目录里所有的文件连同目录全都删掉了（除了 "-rf"）。


===== 表面解决方案 =====



难道这说明我们应该禁止任何以 "-" 开头的文件名的存在，因为这样会让程序分不清选项和文件名？可是不幸的是，由于 Unix 给程序员的“灵活性”，并不是每个程序都认为以 "-" 开头的参数是选项。比如，Linux 下的 tar，ps 等命令就是例外。所以这个方案不大可行。

从上面的例子我们可以看出，问题的来源似乎是因为 ls 根本不知道通配符 * 的存在。是 shell 把通配符展开以后给 ls。其实 ls 得到的是文件名和选项混合在一起的字符串数组。所以 UHH 的作者提出的一个看法：“shell 根本不应该展开通配符。通配符应该直接被送给程序，由程序自己调用一个库函数来展开。”

这个方案确实可行：如果 shell 把通配符直接给 ls，那么 ls 会只看到 "*" 一个参数。它会调用库函数在文件系统里去寻找当前目录下的所有文件，它会很清楚的知道 "-l" 是一个文件，而不是一个选项，因为它根本没有从 shell 那里得到任何选项(它只得到一个参数："*")。所以问题貌似就解决了。

但是这样每一个命令都自己检查通配符的存在，然后去调用库函数来解释它，大大增加了程序员的工作量和出错的概率。况且 shell 不但展开通配符，还有环境变量，花括号展开，~展开，命令替换，算术运算展开…… 这些让每个程序都自己去做？这恰恰违反了第一条 Unix 哲学——模块化原则。而且这个方法并不是一劳永逸的，它只能解决这一个问题。我们还将遇到文本流引起的更多的问题，它们没法用这个方法解决。下面就是一个这样的例子。


===== 冰山又一角 =====



这些看似微不足道的问题里面其实包含了 Unix 本质的问题。如果不能正确认识到它，我们跳出了一个问题，还会进入另一个。我讲一个自己的亲身经历吧。我前年夏天在 Google 实习快结束的时候发生了这样一件事情……

由于我的项目对一个开源项目的依赖关系，我必须在 Google 的 Perforce 代码库中提交这个开源项目的所有文件。这个开源项目里面有 9000 多个文件，而 Perforce 是如此之慢，在提交进行到一个小时的时候，突然报错退出了，说有两个文件找不到。又试了两次（顺便出去喝了咖啡，打了台球），还是失败，这样一天就快过去了。于是我搜索了一下这两个文件，确实不存在。怎么会呢？我是用公司手册上的命令行把项目的文件导入到 Perforce 的呀，怎么会无中生有？这条命令是这样：

find -name *.java -print | xargs p4 add



它的工作原理是，find 命令在目录树下找到所有的以 ".java" 结尾的文件，把它们用空格符隔开做成一个字符串，然后交给 xargs。之后 xargs 以空格符把这个字符串拆开成多个字符串，放在 "p4 add" 后面，组合成一条命令，然后执行它。基本上你可以把 find 想象成 Lisp 里的 "filter"，而 xargs 就是 "map"。所以这条命令转换成 Lisp 样式的伪码就是:

(map (lambda (x) (p4 add x))
     (filter (lambda (x) (regexp-match? "*.java" x))
             (files-in-current-dir)))

问题出在哪里呢？经过一下午的困惑之后我终于发现，原来这个开源项目里某个目录下，有一个叫做 "App Launcher.java" 的文件。由于它的名字里面含有一个空格，被 xargs 拆开成了两个字符串： "App" 和 "Launcher.java"。当然这两个文件都不存在了！所以 Perforce 在提交的时候抱怨找不到它们。我告诉组里的负责人这个发现后，他说：“这些家伙，怎么能给 Java 程序起这样一个名字？也太菜了吧！”

但是我却不认为是这个开源项目的程序员的错误，这其实显示了 Unix 的问题。这个问题的根源是因为 Unix 的命令 (find, xargs) 把文件名以字符串的形式传递，它们默认的“协议”是“以空格符隔开文件名”。而这个项目里恰恰有一个文件的名字里面有空格符，所以导致了歧义的产生。该怪谁呢？既然 Linux 允许文件名里面有空格，那么用户就有权使用这个功能。到头来因此出了问题，用户却被叫做菜鸟，为什么自己不小心，不看 man page。

后来我仔细看了一下 find 和 xargs 的 man page，发现其实它们的设计者其实已经意识到这个问题。所以 find 和 xargs 各有一个选项："-print0" 和 "-0"。它们可以让 find 和 xargs 不用空格符，而用 "NULL"（ASCII字符 0）作为文件名的分隔符，这样就可以避免文件名里有空格导致的问题。可是，似乎每次遇到这样的问题总是过后方知。难道用户真的需要知道这么多，小心翼翼，才能有效的使用 Unix 吗？


===== 文本流不是可靠的接口 =====



这些例子其实从不同的侧面显示了同一个本质的问题：用文本流来传递数据有严重的问题。是的，文本流是一个“通用”的接口，但是它却不是一个“可靠”或者“方便”的接口。Unix 命令的工作原理基本是这样： 


    * 从标准输入得到文本流，处理，向标准输出打印文本流。

    * 程序之间用管道进行通信，让文本流可以在程序间传递。


这其中主要有两个过程：

    - 程序向标准输出“打印”的时候，数据被转换成文本。这是一个编码过程。

    - 文本通过管道（或者文件）进入另一个程序，这个程序需要从文本里面提取它需要的信息。这是一个解码过程。

编码的貌似很简单，你只需要随便设计一个“语法”，比如“用空格隔开”，就能输出了。可是编码的设计远远不是想象的那么容易。要是编码格式没有设计好，解码的人就麻烦了，轻则需要正则表达式才能提取出文本里的信息，遇到复杂一点的编码（比如程序文本），就得用 parser。最严重的问题是，由于鼓励使用文本流，很多程序员很随意的设计他们的编码方式而不经过严密思考。这就造成了 Unix 的几乎每个程序都有各自不同的输出格式，使得解码成为非常头痛的问题，经常出现歧义和混淆。

上面 find/xargs 的问题就是因为 find 编码的分隔符（空格）和文件名里可能存在的空格相混淆——此空格非彼空格也。而之前的 ls 和 rm 的问题就是因为 shell 把文件名和选项都“编码”为“字符串”，所以 ls 程序无法通过解码来辨别它们的到底是文件名还是选项——此字符串非彼字符串也！

如果你使用过 Java 或者函数式语言（Haskell 或者 ML），你可能会了解一些类型理论(type theory)。在类型理论里，数据的类型是多样的，Integer, String, Boolean, List, record…… 程序之间传递的所谓“数据”，只不过就是这些类型的数据结构。然而按照 Unix 的设计，所有的类型都得被转化成 String 之后在程序间传递。这样带来一个问题：由于无结构的 String 没有足够的表达力来区分其它的数据类型，所以经常会出现歧义。相比之下，如果用 Haskell 来表示命令行参数，它应该是这样：

data Parameter = Option String | File String | ...

虽然两种东西的实质都是 String，但是 Haskell 会给它们加上“标签”以区分 Option 还是 File。这样当 ls 接收到参数列表的时候，它就从标签判断哪个是选项，哪个是参数，而不是通过字符串的内容来瞎猜。

===== 文本流带来太多的问题 =====


综上所述，文本流的问题在于，本来简单明了的信息，被编码成为文本流之后，就变得难以提取，甚至丢失。前面说的都是小问题，其实文本流的带来的严重问题很多，它甚至创造了整个的研究领域。文本流的思想影响了太多的设计。比如：

    * 配置文件：几乎每一个都用不同的文本格式保存数据。想想吧：.bashrc, .Xdefaults, .screenrc, .fvwm, .emacs, .vimrc, /etc目录下那系列！这样用户需要了解太多的格式，然而它们并没有什么本质区别。为了整理好这些文件，花费了大量的人力物力。

    * 程序文本：这个以后我会专门讲。程序被作为文本文件，所以我们才需要 parser。这导致了整个编译器领域花费大量人力物力研究 parsing。其实程序完全可以被作为 parse tree 直接存储，这样编译器可以直接读取 parse tree，不但节省编译时间，连 parser 都不用写。

    * 数据库接口：程序与关系式数据库之间的交互使用含有 SQL 语句的字符串，由于字符串里的内容跟程序的类型之间并无关联，导致了这种程序非常难以调试。

    * XML: 设计的初衷就是解决数据编码的问题，然而不幸的是，它自己都难 parse。它跟 SQL 类似，与程序里的类型关联性很差。程序里的类型名字即使跟 XML 里面的定义有所偏差，编译器也不会报错。Android 程序经常出现的 "force close"，大部分时候是这个原因。与 XML 相关的一些东西，比如 XSLT, XQuery, XPath 等等，设计也非常糟糕。

    * Web：JavaScript 经常被作为字符串插入到网页中。由于字符串可以被任意组合，这引起很多安全性问题。Web安全研究，有些就是解决这类问题的。

    * IDE接口：很多编译器给编辑器和 IDE 提供的接口是基于文本的。编译器打印出出错的行号和信息，比如 "102:32 variable x undefined"，然后由编辑器和 IDE 从文本里面去提取这些信息，跳转到相应的位置。一旦编译器改变打印格式，这些编辑器和 IDE 就得修改。

    * log分析: 有些公司调试程序的时候打印出文本 log 信息，然后专门请人写程序分析这种 log，从里面提取有用的信息，非常费时费力。

    * 测试：很多人写 unit test 的时候，喜欢把数据结构通过 toString 等函数转化成字符串之后，与一个标准的字符串进行比较，导致这些测试在字符串格式改变之后失效而必须修改。


还有很多的例子，你只需要在你的身边去发现。


===== 什么是“人类可读”和“通用”接口？ =====



当我提到文本流做接口的各种弊端时，经常有人会指出，虽然文本流不可靠又麻烦，但是它比其它接口更通用，因为它是唯一人类可读 (human-readable) 的格式，任何编辑器都可以直接看到文本流的内容，而其它格式都不是这样的。对于这一点我想说的是： 


    - 什么叫做“人类可读”？文本流真的就是那么的可读吗？几年前，普通的文本编辑器遇到中文的时候经常乱码，要折腾好一阵子才能让它们支持中文。幸好经过全世界的合作，我们现在有了 Unicode。

    - 现在要阅读 Unicode 的文件，你不但要有支持 Unicode 的编辑器/浏览器，你还得有能显示相应码段的字体。文本流达到“人类可读”真的不费力气？

    - 除了文本流，其实还有很多人类可读的格式，比如 JPEG。它可比文本流“可读”和“通用”多了，连字体都用不着。



所以，文本流的根本就不是“人类可读”和“通用”的关键。真正的关键在于“标准化”。如果其它的数据类型被标准化，那么我们可以在任何编辑器，浏览器，终端里加入对它们的支持，完全达到人类和机器都可轻松读取，就像我们今天读取文本和 JPEG 一样。

=====解决方案 =====



其实有一个简单的方式可以一劳永逸的解决所有这些问题： 


    - 保留数据类型本来的结构。不用文本流来表示除文本以外的数据。

    - 用一个开放的，标准化的，可扩展的方式来表示所有数据类型。

    - 程序之间的数据传递和存储，就像程序内部的数据结构一样。


===== Unix 命令行的本质 =====



虽然文本流引起了这么多问题，但是 Unix 还是不会消亡，因为毕竟有这么多的上层应用已经依赖于它，它几乎是整个 Internet 的顶梁柱。所以这篇文章对于当前状况的一个实际意义，也许是可以帮助人们迅速的理解 Unix 的命令行机制，并且鼓励程序员在新的应用中使用结构化的数据。

Unix 命令虽然过于复杂而且功能冗余，但是如果你看透了它们的本质，就能轻而易举的学会它们的使用方法。简而言之，你可以用普通的编程思想来解释所有的 Unix 命令：

    - 函数：每一个 Unix 程序本质上是一个函数 (main)。

    - 参数：命令行参数就是这个函数的参数。 所有的参数对于 C 语言来说都是字符串，但是经过 parse，它们可能有几种不同的类型：

        - 变量名：实际上文件名就是程序中的变量名，就像 x, y。而文件的本质就是程序里的一个对象。

        - 字符串：这是真正的程序中的字符串，就像 "hello world"。

        - keyword argument: 选项本质上就是“keyword argument”(kwarg)，类似 Python 或者 Common Lisp 里面那个对应的东西，短选项（看起来像 "-l", "-c" 等等），本质上就是 bool 类型的 kwarg。比如 "ls -l" 以 Python 的语法就是 ls(l=true)。长选项本质就是 string 类型的 kwarg。比如 "ls --color=auto" 以 Python 的语法就是 ls(color=auto)。

    - 返回值：由于 main 函数只能返回整数类型(int)，我们只好把其它类型 (string, list, record, ...) 的返回值序列化为文本流，然后通过文件送给另一个程序。这里“文件”通指磁盘文件，管道等等。它们是文本流通过的信道。我已经提到过，文件的本质是程序里的一个对象。

    - 组合：所谓“管道”，不过是一种简单的函数组合(composition)。比如 "A x | B"，用函数来表示就是 "B(A(x))"。 但是注意，这里的计算过程，本质上是 lazy evaluation (类似 Haskell)。当 B “需要”数据的时候，A 才会读取更大部分的 x，并且计算出结果送给 B。并不是所有函数组合都可以用管道表示，比如，如何用管道表示 "C(B(x), A(y))"？所以函数组合是更加通用的机制。

    - 分支：如果需要把返回值送到两个不同的程序，你需要使用 tee。这相当于在程序里把结果存到一个临时变量，然后使用它两次。

    - 控制流：main 函数的返回值（int型）被 shell 用来作为控制流。shell 可以根据 main 函数返回值来中断或者继续运行一个脚本。这就像 Java 的 exception。

    - shell: 各种 shell 语言的本质都是用来连接这些 main 函数的语言，而 shell 的本质其实是一个 REPL (read-eval-print-loop，类似 Lisp)。用程序语言的观点，shell 语言完全是多余的东西，我们其实可以在 REPL 里用跟应用程序一样的程序语言。Lisp 系统就是这样做的。


===== 数据直接存储带来的可能性 =====



由于存储的是结构化的数据，任何支持这种格式的工具都可以让用户直接操作这个数据结构。这会带来意想不到的好处。

    - 因为命令行操作的是结构化的参数，系统可以非常智能的按类型补全命令，让你完全不可能输入语法错误的命令。

    - 可以直接在命令行里插入显示图片之类的 "meta data"。

    - Drag&Drop 桌面上的对象到命令行里，然后执行。

    - 因为代码是以 parse tree 结构存储的，IDE 会很容易的扩展到支持所有的程序语言。

    - 你可以在看 email 的时候对其中的代码段进行 IDE 似的结构化编辑，甚至编译和执行。

    - 结构化的版本控制和程序比较(diff)。（参考我的talk）


还有很多很多，仅限于我们的想象力。


===== 程序语言，操作系统，数据库三位一体 =====



如果 main 函数可以接受多种类型的参数，并且可以有 keyword argument，它能返回一个或多个不同类型的对象作为返回值，而且如果这些对象可以被自动存储到一种特殊的“数据库”里，那么 shell，管道，命令行选项，甚至连文件系统都没有必要存在。我们甚至可以说，“操作系统”这个概念变得“透明”。因为这样一来，操作系统的本质不过是某种程序语言的“运行时系统”(runtime system)。这有点像 JVM 之于 Java。其实从本质上讲，Unix 就是 C 语言的运行时系统。

如果我们再进一步，把与数据库的连接做成透明的，即用同一种程序语言来“隐性”(implicit)的访问数据库，而不是像 SQL 之类的专用数据库语言，那么“数据库”这个概念也变得透明了。我们得到的会是一个非常简单，统一，方便，而且强大的系统。这个系统里面只有一种程序语言，程序员直接编写高级语言程序，用同样的语言从命令行执行它们，而且不用担心数据放在什么地方。这样可以大大的减小程序员工作的复杂度，让他们专注于问题本身，而不是系统的内部结构。

实际上，类似这样的系统在历史上早已存在过 (Lisp Machine, System/38，Oberon)，而且收到了不错的效果。但是由于某些原因（历史的，经济的，政治的，技术的），它们都消亡了。但是不得不说它们的这种方式比 Unix 现有的方式优秀，所以何不学过来？我相信，随着程序语言和编译器技术发展，它们的这种简单而统一的设计理念，有一天会改变这个世界。

====== 所谓软件工程 ======


很多编程的人包括我，头衔叫做“软件工程师”（software engineer），然而我却不喜欢这个名字。我喜欢把自己叫做“程序员”（programmer）或者“计算机科学家”（computer scientist）。这是为什么呢？这需要从“软件工程”（software engineering）在现实中的涵义谈起。

有人把软件工程这个领域的本质总结为：“How to program if you cannot？”（如果你不会编程，那么你如何编程？）我觉得这句话说得很好，因为我发现软件工程这整个领域，基本就是吹牛扯淡卖“减肥药”的。软件行业的大部分莫名其妙的愚昧行为，很多是由所谓“软件工程专家”发明的。总有人提出一套套的所谓“方法论”或者“原则”，比如Extreme Programming，Design Patterns，Agile，Pair Programming，Test Driven Development（TDD），DRY principle，…… 他们把这些所谓方法论兜售给各个软件公司，鼓吹它们的各种好处，说使用这些方法，就可以用一些平庸的“软件工程师”，制造出高质量低成本的软件。这就跟减肥药的广告一样：不用运动，不用节食，一个星期瘦20斤。你开头还不以为然，觉得这些肤浅的说法能造成什么影响。结果久而久之，这些所谓“方法论”和“原则”成为了整个行业的教条，造成了文化大革命一样的风气。违反这些教条的人，必然被当成菜鸟一样的鄙视，当成小学生一样的教育，当成“反革命”一样的批斗。就算你技术比这些教条的提出者还高明不知道多少倍也一样。

打破这些软件工程专家们制造的幻觉的一个办法，就是实地去看看这些所谓专家们自己用这些方法论做出了什么好东西。你会惊奇的发现，这些提出各种玄乎其玄的新名词的所谓“专家”，几乎都是从不知道什么旮旯里冒出来的民科，没有一个做出过什么有技术含量的东西，他们根本没有资格对别人编程的方式做出指导。这些人做出来少数有点用的东西（比如JUnit），其实非常容易，以至于每个初学编程的人都应该做得出来。可世界上就是有这样划算的职业，你虽然写不出好的代码，你对计算原理的理解非常肤浅，却可以通过一些手段，得到评价别人的“代码质量”的权力，占据软件公司的管理层位置。久而久之，别人还以为你是什么泰斗。你仔细看过提出Java Design Pattern的四个人（GoF），到底做出过什么厉害的东西吗？没有。提出“DRY Principle”的作者，做出过什么好东西吗？没有。再看看Agile，Pair Programming，TDD……的提出者？全都是一群饭桶。他们其实根本就不懂很多编程的东西，写出文章和书来也是极其肤浅，一知半解。

所谓“软件工程”，并不像土木工程，机械工程，电机工程，是建立在实际的，科学的基础上的。跟这些“硬工程”不一样，软件弄得不好不会出人命，也不会跟做芯片的公司那样，出一个bug立即导致上亿的损失，身败名裂。所以研究软件工程，似乎特别容易钻空子，失败了之后容易找借口和替罪羊。如果你说我的方法不好，你有什么证据吗？口说无凭，我浪费了你多少时间呢？你的具体执行是不是完全照我说的来的呢？你肯定有什么细节没按我说的做，所以才会失败。总之，如果你用了我的办法不管用，那是你自己的问题！

想起这些借口我就想起一个笑话：两夫妻睡觉发现床上有跳蚤，身上被咬了好多大包。去买了号称“杀伤率100%”的跳蚤药，撒了好多在床上。第二天早上起来，发现又被咬了好多新的大包。妻子责怪丈夫，说他没看说明书就乱撒。结果丈夫打开说明书一看，内容如下：

    本跳蚤药使用方法：

        - 抓住跳蚤
        - 掰开跳蚤的嘴
        - 把药塞进跳蚤嘴里
        - 合上跳蚤的嘴

我发现很多软件工程的所谓方法论失败之后的借口，跟这跳蚤药的说明书很像 :)

人都想省钱，雇用高质量的程序员不容易呀，所以很多公司还是上钩了。他们请这些“软件工程专家”来到公司，推行各种各样的软件方法论，可是发现最后都失败了。这是为什么呢？因为再高明的方法论，也无法代替真正的，精华的计算机科学教育。直到今天还有很多公司推行所谓的Agile，煞有介事的搞一些stand-up meeting, scrum之类的形式主义东西，以为这些过家家似的做法就能提高开发质量和效率。很多开发人员也很把一些软件工程的工具当回事，喜欢折腾Git，Maven等工具一些偏僻的“新功能”。他们很在乎所谓的版本控制，测试等东西，以为熟练的掌握这些就能开发出高质量，可靠的代码。可是你最后发现，无论你如何高效的使用这些工具，它们都只能起到辅助的，次要的作用。编程工具永远不是程序本身，对编程工具的熟练掌握，永远也无法代替真正的对程序和计算的理解。过分强调这些工具的使用，是本末倒置的，让工程走上失败道路的作法。

编程真的是一门艺术，它完全符合艺术的各种特征，编程界也充满了艺术界的独有特征。有些初学艺术的人（比如10年前的我），总是挑剔手上的工具，非要用最新最炫的工具，用它们最偏僻最难用的“特性”，才觉得自己能够做出优秀的作品。很多人照不出好的照片，就怪相机不好。买了几万块钱的笨重高档相机，照出来的照片还不如别人用手机照的。这些人不明白，好的摄影师和不好的摄影师，区别在于眼睛，而不是相机。一个真正的艺术家，可以用任何在手上的工具创造出色的作品。有些甚至可以用一些废品垃圾，拙劣的工具，做出杰出的，别具风味的艺术品。因为艺术存在于人的心里，而不在他们使用的工具里面。

====== DRY原则的误区 ======


很多编程的人，喜欢鼓吹各种各样的“原则”，比如KISS原则，DRY原则…… 总有人把这些所谓原则奉为教条或者秘方，以为兢兢业业地遵循这些，空喊几个口号，就可以写出好的代码。同时，他们对违反这些原则的人嗤之以鼻——你不知道，不遵循或者藐视这些原则，那么你就是菜鸟。所谓“DRY原则”（Don't Repeat Yourself，不要重复你自己）就是这些教条其中之一。盲目的迷信DRY原则，在实际的工程中带来了各种各样的问题，却经常被忽视。

简言之，DRY原则鼓励对代码进行抽象，但是鼓励得过了头。DRY原则说，如果你发现重复的代码，就把它们提取出去做成一个“模板”或者“框架”。对于抽象我非常的在行，实际上程序语言专家做的许多研究，就是如何设计更好的抽象。然而我并不奉行所谓DRY原则，并不是尽一切可能避免“重复”。“避免重复”并不等于“抽象”。有时候适当的重复代码是有好处的，所以我有时候会故意的进行重复。
===== 抽象与可读性的矛盾 =====


代码的“抽象”和它的“可读性”（直观性），其实是一对矛盾的关系。适度的抽象和避免重复是有好处的，它甚至可以提高代码的可读性，然而如果你尽“一切可能”从代码里提取模板，甚至把一些微不足道的“共同点”也提出来进行“共享”，它就开始有害了。这是因为，模板并不直接显示在“调用”它们的位置。提取出模板，往往会使得阅读代码时不能一目了然。如果由此带来的直观性损失超过了模板所带来的好处时，你就应该考虑避免抽象了。要知道，代码读的次数要比写的次数多很多。很多人为了一时的“写的快感”，过早的提取出不必要的模板，其实损失了读代码时的直观性。如果自己的代码连自己都不能一目了然，你就不能写出优雅的代码。

举一个实际的例子。奉行DRY原则的人，往往喜欢提取类里面的“共同field”，把它们放进一个父类，然后让原来的类继承这个父类。比如，本来的代码可能是：

  class A {
    int a;
    int x;
    int y;
  }

  class B {
    int a;
    int u;
    int v;
  }

奉行DRY原则的人喜欢把它改成这样：

  class C {
    int a;
  }

  class A extends C {
    int x;
    int y;
  }

  class B extends C {
    int u;
    int v;
  }

后面这段代码有什么害处呢？它的问题是，当你看到class A和class B的定义时，你不再能一目了然的看到int a这个field。“可见性”，对于程序员能够产生直觉，是非常重要的。这种无关紧要的field，其实大部分时候都没必要提出去，造出一个新的父类。很多时候，不同类里面虽然有同样的int a这样的field，然而它们的含义却是完全不同的。有些人不管三七二十一就来个“DRY”，结果不但没带来好处，反而让程序难以理解。
===== 抽象的时机问题 =====


奉行DRY原则的人还有一个问题，就是他们随时都在试图发现“将来可能重用”的代码，而不是等到真的出现重复的时候再去做抽象。很多时候他们提取出一个貌似“经典模板”，结果最后过了几个月发现，这个模板在所有代码里其实只用过一次。这就是因为他们过早的进行了抽象。

抽象的思想，关键在于“发现两个东西是一样的”。然而很多时候，你开头觉得两个东西是一回事，结果最后发现，它们其实只是肤浅的相似，而本质完全不同。同一个int a，其实可以表示很多种风马牛不及的性质。你看到都是int a就提出来做个父类，其实反而让程序的概念变得混乱。还有的时候，有些东西开头貌似同类，后来你增添了新的逻辑之后，发现它们的用途开始特殊化，后来就分道扬镳了。过早的提取模板，反而捆住了你的手脚，使得你为了所谓“一致性”而重复一些没用的东西。这样的一致性，其实还不如针对每种情况分别做特殊处理。

防止过早抽象的方法其实很简单，它的名字叫做“等待”。其实就算你不重用代码，真的不会死人的。时间能够告诉你一切。如果你发现自己仿佛正在重复以前写过代码，请先不要停下来，请坚持把这段重复的代码写完。如果你不把它写出来，你是不可能准确的发现重复的代码的，因为它们很有可能到最后其实是不一样的。

你还应该避免没有实际效果的抽象。如果代码才重复了两次，你就开始提取模板，也许到最后你会发现，这个模板总共也就只用了两次！只重复了两次的代码，大部分时候是不值得为它提取模板的。因为模板本身也是代码，而且抽象思考本身是需要一定代价的。所以最后总的开销，也许还不如就让那两段重复的代码待在里面。

这就是为什么我喜欢一种懒懒的，笨笨的感觉。因为我懒，所以我不会过早的思考代码的重用。我会等到事实证明重用一定会带来好处的时候，才会开始提取模板，进行抽象。经验告诉我，每一次积极地寻找抽象，最后的结果都是制造一些不必要的模板，搞得自己的代码自己都看不懂。很多人过度强调DRY，强调代码的“重用”，随时随地想着抽象，结果被这些抽象搅混了头脑，bug百出，寸步难行。如果你不能写出“可用”（usable）的代码，又何谈“可重用”（reusable）的代码呢？
===== 谨慎的对待所谓原则 =====


说了这么多，我是在支持DRY，还是反对DRY呢？其实不管是支持还是反对它，都会表示我在乎它，而其实呢，我完全不在乎这类原则，因为它们非常的肤浅。这就像你告诉我说你有一个重大的发现，那就是“1+1=2”，我该支持你还是反对你呢？我才懒得跟你说话。人们写程序，本来自然而然就会在合适的时候进行抽象，避免重复，怎么过了几十年后，某个菜鸟给我们的做法起了个名字叫DRY，反而他成了“大师”一样的人物，我倒要用“DRY”这个词来描述我一直在干的事情呢？所以我根本不愿意提起“DRY”这个名字。

所以我觉得这个DRY原则根本就不应该存在，它是一个根本没有资格提出“原则”的人提出来的。看看他鼓吹的其它低劣东西（比如Agile，Ruby），你就会发现，他是一个兜售减肥药的“软件工程专家”。世界上有太多这样的肤浅的所谓原则，我不想对它们一一进行评价，这是在浪费我的时间。世界上有比这些喜欢提出“原则”的软件工程专家深邃很多的人，他们懂得真正根本的原理。

====== 谈“测试驱动的开发” ======


现在的很多公司，包括 Google 和我现在的公司 Coverity，都喜欢一种“测试驱动的开发”（test-driven development）。它的原理是，在写程序的时候同时写上自动化的“单元测试”（unit test）。在代码修改之后，这些测试可以批量的被运行，这样就可以避免不应该出现的错误。

这不是一个坏主意。我在 Kent 的编译器课程上也使用了很多测试。它们在编译器的开发中是不可缺少的。编译器是一种极其精密的程序，微小的改动都可能带来重大的错误。所以编译器的项目一般都含有大量的测试。

然而测试的构建，应该是在程序主体已经成形的情况下才能进行。如果程序属于创造性的设计，主体并未成形，过早的加入测试反而会大幅度的降低开发效率。所以当我给 Google 开发 Python 静态分析的时候，我几乎没有使用任何测试。虽然组里的成员催我写测试，但是我却知道那只会降低我的开发效率，因为这个程序在几个星期的过程中，被我推翻重来了好几次。要是我一开头就写上测试，这些测试就会碍手碍脚，阻碍我大幅度的修改代码。

测试的另一个副作用是，它让很多人对测试有一种盲目的依赖心理。改了程序之后，把测试跑一遍没出错，就以为自己的代码是正确的。可是测试其实并不能保证代码的正确，即使完全“覆盖”了也是一样。覆盖只是说你的代码被测试碰到过了，可是它在什么条件下碰到的却没法判断。如果实际的条件跟测试时的条件不同，那么实际运行中仍然会出问题。测试的条件往往是“组合爆炸”的数量级，所以你不可能测试所有的情况。唯一能可靠的方法是使用严密的“逻辑推理”，证明它的正确。

当然我并不是让你用 ACL2 或者 Coq 这样的定理证明软件。虽然它们的逻辑非常严密，但是用它们来证明复杂的软件系统，需要顶尖的程序员和大量的时间。即使如此，由于理论的限制，程序的正确性有可能根本无法证明。所以我这里说的“逻辑推理”，只是局部的，人力的，基本的逻辑推理。

很多人写程序只是凭现象来判断，而不能精密的分析程序的逻辑，所以他们修改程序经常“治标不治本”。如果程序出问题了，他们的办法是看看哪里错了，也不怎么理解，就改一下让它不再出错，最多再把所有测试跑一遍。或者再加上一些新的测试，以保证这个地方下次不再出问题。

这种做法的结果是，程序里出现大量的“特殊情况”和“创可贴”。把一个“虫子”按下去，另一个虫子又冒出来。忙活来忙活去，最后仍然不能让程序满足“所有情况”。其实能够“满足所有情况”的程序，往往比能够“满足特殊情况”的程序简单很多。这是一个很奇怪的事情：能做的事越多，代码量却越少。也许这就叫做程序的“美”，它跟数学的“美”其实是一回事。

美的程序不可能从修修补补中来。它必须完美的把握住事物的本质，否则就会有许许多多无法修补的特例。其实程序员跟画家差不多，画家如果一天到头蹲在家里，肯定什么好东西也画不出来。程序员也一样，蹲在家里面对电脑，其实很难写出什么好的代码。你必须出去观察事物，寻找“灵感”，而不只是写代码。在修改代码的时候，你必须用“心灵之眼”看见代码背后所表达的事物。这也是为什么很多高明的程序员不怎么用调试器（debugger）的原因。他们只是用眼睛看着代码，然后闭上眼，脑海里浮现出其中信息的流动，所以他们经常一动手就能改到正确的地方。

====== 谈程序的“通用性” ======


在现实的软件工程中，我经常发现这样的一种现象。本来用很简单的代码就可以解决的问题，却因为设计者过分的关注了“通用性”，“可维护性”和“可扩展性”，被搞得绕了几道弯，让人琢磨不透。

这些人的思维方式是这样的：“将来这段代码可能会被用到更多的场合，所以我现在就考虑到扩展问题。”于是乎，他们在代码中加入了各种各样的“框架结构”，目的是为了在将来有新的需要的时候，代码能够“不加修改”就被用到新的地方。

我并不否认“通用性”的价值，实际上我的某些程序通用性非常之强。可是很多人所谓的“通用性”，其实达到的是适得其反的效果。这种现象通常被称为“过度工程” (over-engineer)。关于过度工程，有一个有趣的故事：

http://www.snopes.com/business/genius/spacepen.asp

    传说 1960 年代美俄“太空竞赛”的时候，NASA 遇到一个严重的技术问题：宇航员需要一支可以在外太空的真空中写字的钢笔。最后 NASA 耗资150万美元研制出了这样的钢笔。可惜这种钢笔在市场上并不行销。

    俄国人也遇到同样的问题。他们使用了铅笔。

这个故事虽然是假的，但是却具有伊索寓言的威力。现在再来看我们的软件行业，你也许会发现：

===== 代码需要被“重用”的场合，实际上比你想象的要少 =====

    我发现很多人写程序的时候连“眼前特例”都没做好，就在开始“展望将来”。他们总是设想别人会重用这段代码。而实际上，由于他们的设计过于复杂，理解这设计所需的脑力开销已经高于从头开始的代价，所以大部分人其实根本不会去用他们的代码，自己重新写一个就是了。也有人到后来发现，之前写的那段代码，连自己都看不下去了，恨不得删了重来，就不要谈什么重用了。

====== 修改代码所需要的工作实际上比你想象的要少 ======


    还有一种情况是，这些被设计来“共享”的代码，其实根本没有被用在很多的地方，所以即使你完全手动的修改它们也花不了很多时间。现在再加上 IDE 技术的发展和各种先进的 refactor 工具，批量的修改代码已经不是特别麻烦的事情。曾经需要在逻辑层面上进行的可维护性设计，现在有可能只需要在 IDE 里面点几下鼠标就轻松完成。所以在考虑设计一个框架之前，你应该同时考虑到这些因素。

====== “考虑”到了通用性，并不等于你就准确地“把握”住了通用性 ======


    很多人考虑到了通用性，却没有准确的看到，到底是哪一个部分将来可能需要修改，所以他们的设计经常抓不住关键。当有新的需要出现的时候，才发现原来设想的可能变化的部分，其实根本没有变，而原来以为不会变的地方却变了。

    能够准确的预测将来的需要，能够从代码中抽象出真正通用的框架，是一件非常困难的事情。它不止需要有编程的能力，而且需要对真实世界里的事物有强大的观察能力。很多人设计出来的框架，其实只是照搬别人的经验，却不能适应实际的需要。在 Java 世界里的很多 design pattern，就是这些一知半解的人设计出来的。

====== 初期设计的复杂性 ======


    如果在第一次的设计中就过早的考虑到将来，由此带来的多余的复杂性，有可能让初期的设计就出现问题。所以这种对于将来的变化的考虑，实际上帮了倒忙。本来如果专注于解决现在的问题，能够得到非常好的结果。但是由于“通用性”带来的复杂度，设计者的头脑每次都要多转几道弯，所以它无法设计出优雅的程序。

====== 理解和维护框架性代码的开销 ======


    如果你设计了框架性的代码，每个程序员为了在这个框架下编写代码，都需要理解这种框架的构造，这带来了学习的开销。一旦发现这框架有设计问题，依赖于它的代码很有可能需要修改，这又带来了修改的开销。所以加入“通用性”之后，其实带来了更多的工作。这种开销能不能得到回报，依赖于以上的多种因素。

所以在设计程序的时候，我们最好是先把手上的问题解决好。如果发现这段代码还可以被用在很多别的地方，到时候再把框架从中抽象出来也不迟。

====== 解密“设计模式”======
有些人问我，你说学习操作系统的最好办法是学习程序设计。那我们是不是应该学习一些“设计模式”（design patterns）。这是一个我很早就有定论，而且经过实践检验的问题，所以想在这里做一个总结。

总的来说，如果光从字面上讲，程序里确实是有一些“模式”可以发掘的。因为你总是可以借鉴以前的经验，用来构造新的程序。你可以把这种经验叫做“模式”。可是自从《设计模式》（通常叫做 GoF，“Gang of Four”，“四人帮”）这本书在 1994 年发表以来，“设计模式”这个词有了新的，扭曲的含义。它变成了一种教条，带来了公司里程序的严重复杂化以及效率低下。



GoF 借鉴的是一个叫 Christopher Alexander 的建筑师的做法。Alexander 给一些建筑学里的“设计模式”起了名字，试图让建筑师们有一些“共同语言”。可惜的是，Alexander 后来自己都承认，他的实验失败了。因为这些固定的模式，并没能有效地传递精髓的知识，没能让新手成长为出色的建筑师。

照搬模式东拼西凑，而不能抓住事物的本质，没有“灵感”，其实是设计不出好东西的。这就像照搬“模版”把作文写得再好，也成不了作家一样。

我孤陋寡闻，当听说这本书的时候，我已经学会了函数式编程，正在 Cornell 读 PhD，专攻程序语言设计。有一天由于好奇这书为什么名气这么大，我从图书馆借了一本回来看。我很快的发现，其实这本书的作者只是给早已经存在的编程方法起了一些新的名字而已。当时我就拿起一张纸，把所有的20来个设计模式跟我常用的编程概念做了一个映射。这个映射居然是“多对一”（many-to-one）的。也就是说，多个 GoF 设计模式，居然只对应同一个我每天都用的概念。有些概念是如此的不值一提，以至于我根本不需要一个名字来描述它，更不要说多个名字！

其中极少数值得一提的“模式”，也许是 visitor 和 interpreter。很可惜的是，只有很少的人明白如何使用它们。所谓的 visitor，本质上就是函数式语言里的含有“模式匹配”（pattern matching）的递归函数。在函数式语言里，这是多么轻松的事情。可是因为 Java 没有模式匹配，所以很多需要类似功能的人就得使用 visitor pattern。为了所谓的“通用性”，他们往往把 visitor pattern 搞出多层继承关系，让你转几道弯也搞不清楚到底哪个 visitor 才是干实事的。

其实，函数式语言的研究者们早就知道 visitor pattern 是怎么得来的。如果你想知道如何从无到有，一步一步“发明”出 Java 的 visitor pattern，可以参考 Dan Friedman 跟他的学生 Matthias Felleisen 合写的的另一本“小人书”《A Little Java, A Few Patterns》（发表于 1997 年）。



而 interpreter （解释器）模式呢？看了作者们写的例子程序之后，我发现他们其实并不会写解释器，或者说他们不知道如何写出优雅的，正确的解释器。如果你想知道如何写出好的解释器，可以参考我的博文《怎样写一个解释器》。

你说我在贬低这本书的真正价值，因为 GoF 说了：“我们的贡献，就是给这些编程方式起名字。这样让广大程序员有共同的语言。” 如果这也叫贡献的话，我就可以写本书，给“空气”，“水”，“猪肉”这些东西全都起个新名字，让大家有“共同的语言”。这不是搞笑吗。

这不是我的一家之言，Peter Norvig 在 1998 年就做了一个演讲，指出在“动态语言”里面，GoF 的20几个模式，其中绝大部分都“透明”了。也就是说，你根本感觉不到它们的存在。这就像我刚才告诉你的。



在这里 Norvig 的观点是正确的，不过需要小心一个概念错误。Norvig 对“静态语言”的概念是有局限性的。有的静态语言其实也能传递函数作为参数，而且不像 Java 那样什么都得放进 class 里。这样的静态语言，其实也可以避免大部分 GoF 设计模式。而“动态语言”这个概念，在程序语言的理论里面，其实是没有明确的定义的。“动态语言”其实也能进行某些“静态类型检查”。不过在 1998 年，我还是个啥都不懂的屁孩，所以这里就不跟 Norvig 大叔计较了。

既然老人们都有历史局限性，那么为啥我还跟 GoF 找茬？本来这本书很老了，如果没有人再被它误导的话，这篇博文也就不必存在了。可是当我在 Google 实习的时候，我发现几乎每个程序员的书架上都有一本 GoF！我在 Google 实习了两次，第一次的时候代码全都是我一个人写的，所以没有使用任何 GoF 设计模式。代码直接，精巧而简单。当我第二次回到 Google，发现我的代码里已经被加入了各种 factory，visitor，…… 其实啥好事也没做，只不过让我的代码弯了几道弯，让人难以理解。

可见一本坏书，毁掉的不只是一代程序员。鉴于如此，特发此文。各位新手，希望你们敲响警钟，不要再走上这条老路，写出代码来让大家痛苦。

====== 谈程序的正确性 ======


不管在学术圈还是在工业界，总有很多人过度的关心所谓“程序的正确性”，有些甚至到了战战兢兢，舍本逐末的地步。下面举几个例子：

    * 很多人把测试（test）看得过于重要。代码八字还没一撇呢，就吵着要怎么怎么严格的测试，防止“将来”有人把代码改错了。这些人到后来往往被测试捆住了手脚，寸步难行。不但代码bug百出，连测试里面也很多bug。
    * 有些人对于“使用什么语言”这个问题过度的在乎，仿佛只有用最新最酷，功能最多的语言，他们才能完成一些很基本的任务。这种人一次又一次的视一些新语言为“灵丹妙药”，然后一次又一次的幻灭，最后他们什么有用的代码也没写出来。
    * 有些人过度的重视所谓“类型安全”（type safety），经常抱怨手头的语言缺少一些炫酷的类型系统功能，甚至因此说没法写代码了！他们没有看到，即使缺少一些由编译器静态保障的类型安全，代码其实一点问题都没有，而且也许更加简单。
    * 有些人走上极端，认为所有的代码都必须使用所谓“形式化方法”（formal methods），用机器定理证明的方式来确保它100%的没有错误。这种人对于证明玩具大小的代码乐此不疲，结果一辈子也没写出过能解决实际问题的代码。

100%可靠的代码，这是多么完美的理想！可是到最后你发现，天天念叨着要“正确性”，“可靠性”的人，几乎总是眼高手低，说的比做的多。自己没写出什么解决实际问题的代码，倒是很喜欢对别人的“代码质量”评头论足。这些人自己的代码往往复杂不堪，喜欢使用各种看似高深的奇技淫巧，用以保证所谓“正确”。他们的代码被很多所谓“测试工具”和“类型系统”捆住手脚，却仍然bug百出。到后来你逐渐发现，对“正确性”的战战兢兢，其实是这些人不解决手头问题的借口。

===== 衡量程序最重要的标准 =====


这些人其实不明白一个重要的道理：你得先写出程序，才能开始谈它的正确性。看一个程序好不好，最重要的标准，是看它能否有效地解决问题，而不是它是否正确。如果你的程序没有解决问题，或者解决了错误的问题，或者虽然解决问题但却非常难用，那么这程序再怎么正确，再怎么可靠，都不是好的程序。

正确不等于简单，不等于优雅，不等于高效。一个不简单，不优雅，效率低的程序，就算你费尽周折证明了它的正确，它仍然不会很好的工作。这就像你得先有了房子，才能开始要求房子是安全的。想想吧，如果一个没有房子的流浪汉，路过一座没有人住的房子，他会因为这房子“不是100%安全”，而继续在野外风餐露宿吗？写出代码就像有了房子，而代码的正确性，就像房子的安全性。写出可以解决问题的程序，永远是第一位的。而这个程序的正确性，不管它如何的重要，永远是第二位的。对程序的正确性的强调，永远不应该高于写出程序本身。

每当谈起这个问题，我就喜欢打一个比方：如果“黎曼猜想”被王垠证明出来了，它会改名叫“王垠定理”吗？当然不会。它会被叫做“黎曼定理”！这是因为，无论一个人多么聪明多么厉害，就算他能够证明出黎曼猜想，但这个猜想并不是他最先想出来的。如果黎曼没有提出这个猜想，你根本不会想到它，又何谈证明呢？所以我喜欢说，一流的数学家提出猜想，二流的数学家证明别人的猜想。同样的道理，写出解决问题的代码的人，比起那些去证明（测试）他的代码正确性的人，永远是更重要的。因为如果他没写出这段代码，你连要证明（测试）什么都不知道！
如何提高程序的正确性

话说回来，虽然程序的正确性相对于解决问题，处于相对次要的地位，然而它确实是不可忽视的。但这并不等于天天鼓吹要“测试”，要“形式化证明”，就可以提高程序的正确性。

如果你深入研究过程序的逻辑推导就会知道，测试和形式化证明的能力都是非常有限的。测试只能测试到最常用的情况，而无法覆盖所有的情况。别被所谓“测试覆盖”（test coverage）给欺骗了。一行代码被测试覆盖而没有出错，并不等于在那里不会出错。一行代码是否出错，取决于在它运行之前所经过的所有条件。这些条件的数量是组合爆炸关系，基本上没有测试能够覆盖所有这些前提条件。

形式化方法对于非常简单直接的程序是有效的，然而一旦程序稍微大点，形式化方法就寸步难行。你也许没有想到，你可以用非常少的代码，写出Collatz Conjecture这样至今没人证明出来的数学猜想。实际使用中的代码，比这种数学猜想要复杂不知道多少倍。你要用形式化方法去证明所有的代码，基本上等于你永远也没法完成项目。

那么提高程序正确性最有效的方法是什么呢？在我看来，最有效的方法莫过于对代码反复琢磨推敲，让它变得简单，直观，直到你一眼就可以看得出它不可能有问题。

====== 关系模型的实质 ======


每当我批评关系式数据库，就会有人说，SQL和关系式数据库的设计，其实偏离了E.F.Codd最初的关系式理论。关系式理论和关系式模型，本身还是很好的，只不过被人实现的时候搞砸了。如果你看透了本质，就会发现这只是一个托词。关系式数据库的问题是根源性的，这个问题其实源自关系式理论本身，而不只是具体的实现。

人们总是喜欢制造这些概念上的壁垒，用以防止自己的理论受到攻击。把过错推到SQL或者IBM身上，是关系式数据库领域常见的托词，用以掩盖其本质上的空洞和设计上的失误。在下面的讨论里为了方便，我会使用少量SQL来表示关系模型里面对应的概念，但这并不削弱我对关系模型的批评，因为它们表示的是关系式模型里面的核心概念。
关系式模型与数据结构

很多人把关系式理论和数据库独立开来，认为它们是完全不同的领域。而其实，数据结构的理论，可以很容易的解释所有关系式数据库里面的操作。

关系模型的每一个“关系”或者“行”（row），表示的不过是一个普通语言里的“结构”，就像C语言的struct。一个表（table），其实不过是某种结构的数组。举个例子，以下SQL语句构造的数据库表：

  CREATE TABLE Students ( sid CHAR(20),
                        name CHAR(20),
                        login CHAR(20),
                        age INTEGER,
                        gpa REAL )

其实相当于以下C语言的结构数组：

  struct student {
    char* sid;
    char* name;
    char* login;
    int age;
    double gpa;
  }

每一个“foreign key”，其实就是一个指针。每一个join操作，本质上就是对指针的“访问”，找到它所指向的对象。在实现上，join跟指针引用有一定差别，因为 join需要查“索引”（index），所以它比指针引用要慢。

所谓的查询（query），本质上就是函数式语言里面的filter, map等操作。只不过关系式代数更加笨拙，组合能力很弱。比如，以下的SQL语句

  SELECT Book.title
   FROM Book
   WHERE price > 100

其实相当于以下的Lisp代码：

  (map .title
     (filter (lambda (b) (> (.price b) 100)) Book)

===== 关系式模型的局限性 =====


所以关系模型所能表达的东西，其实不会超过普通程序语言所用的数据结构，然而关系模型，却具有比数据结构更多的局限。由于“行”只能有固定的宽度，所以导致了你没法在里面放进任何“变长”的对象。比如，如果你有一个数组，那你是不能把它放在一个行里的。你需要把数组拿出来，旋转90度，做成另一个表B。从原来的表A，用一个“foreign key”指向B。这样在表B的每一行，这个key都要被重复一次，产生大量冗余。这种做法通常被叫做normalization。

这种方法虽然可行，其实它只不过是绕过了关系式模型无须有的限制。类似这样的操作，导致了关系式数据库的繁琐。说白了，normalization就是在手动做一些比C语言的手动内存管理还要低级的工作。连C这么低级的语言，都允许你在结构里面嵌套数组，而在关系式模型里面你却不能。很多宝贵的人力，就是在构造，释放，连接这些“中间表格”的工作中消磨掉了。

另外有一些人（比如这篇文章）通过关系模型与其它数据模型（比如网状模型之类）的对比，以支持关系模型存在的必要性，然而如果你理解了这小节的所有细节就会发现，使用基本的数据结构，其实可以完全的表示关系模型以及被它所“超越”的那些数据模型。这些所谓“数据模型”其实全都是故弄玄虚，无中生有。数据模型可以完全被普通的数据结构所表示，然而它们却不可能表达数据结构带有的所有信息。这些模型之所以流行，是因为它们让人误以为知道了所谓的“一对一”，“一对多”等冠冕堂皇的概念，就可以取代设计数据结构所需要的技能，所以我认为数据模型本身就属于技术上的“减肥药”。
===== NoSQL =====


SQL和关系模型所引起的一系列无须有的问题，终究引发了所谓“NoSQL运动”。很多人认为NoSQL是划时代的革命，然而在我看来，它最多可以被称为“不再愚蠢”。大多数NoSQL数据库的设计者，并没有看到上述的问题，或者他们其实也想故弄玄虚，所以NoSQL数据库的设计，并没有完全摆脱关系模型，以及SQL所带来的思维枷锁。

最早试图冲破关系模型和SQL限制的一种技术，叫做“列模式数据库”（column-based database），比如Vertica, HBase等。这种数据库其实就是针对了我刚刚提到的，关系模型无法保存变长结构的问题。它们所谓的“列压缩”，其实不过是在“行结构”里面增加了对“数组”的表示和实现。很显然，每一个数组需要一个字段来表示它的长度N，剩下的空间用来依次保存每一个元素，这样你只需要一个key就可以找到数组里所有的元素，而不需要把key重复N遍。

这种“巧妙”的实现，也就是你在列模式数据库的广告里看到的，只不过那些广告把它说得天花烂坠，貌似与众不同而已。在列模式数据库里，你不需要进行normalization，也不需要重复很多key。这种对数组的表示，是一开始就应该有的，却被关系模型排除在外。然而，很多列模式数据库并没有看到这一实质。它们经常设定一些无端的限制，比如给变长数组的嵌套层数作出限制，等等。所以，列模式数据库，其实没能完全逃脱关系式数据库的思想枷锁。如此明显的事情，数据库专家们最开头恁是看不到。到后来改来改去改得六成对，还美其名曰“优化”和“压缩”。

最新的一些NoSQL数据库，比如Neo4j, MongoDB等，部分的改善了SQL的表达力问题。Neo4j设计了个古怪的查询语言叫Cypher，不但语法古怪，表达力弱，而且效率出奇的低，以至于几乎任何有用的操作，你都必须使用Java写“扩展”（extension）来完成。MongoDB使用JSON来表示查询，本质就是手写编译器里的语法树（AST），非常奇葩和苦逼。现在看来，数据库的主要问题，其实是语言设计的问题。NoSQL数据库的领域，由于缺乏经过正规教育的程序语言专家，而且由于利益驱使，不尊重事实，所以会在很长一段时间之内处于混沌之中。

其实数据库的问题哪有那么困难，它其实跟“远过程调用”（RPC）没什么两样。只要你有一个程序语言，你就可以发送这语言的代码，到一个“数据服务器”。服务器接受并执行这代码，对数据进行索引，查询和重构，最后返回结果给客户端。如果你看清了SQL的实质，就会发现这样的“过程式设计”，其实并不会损失SQL的“描述”能力。反而由于过程式语言的简单，直接和普遍，使得开发效率大大提高。NoSQL数据库比起SQL和关系式数据库，存在某种优势，也就是因为它们在朦胧中朝着这个“RPC”的方向发展。


