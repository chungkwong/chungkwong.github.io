---
title:  "机器学习实用指南"
layout: post
mathjax: true
tags: 人工智能 机器学习
---

机器学习

## 基本概念

映射是数学最核心的概念，众多问题（但不是全部）都可以视为映射：
- 人脸识别可以视为一个从人脸图片集到人集的映射，把人脸图片对应到人脸所属的人
- 机器翻译可以视为一个从字符串集到字符串集的映射，把来自一个语言的字符串对应到另一语言中语义相同的字符串

对于给定的映射$f:X\to Y$，我们希望对于每个定义域中的值$x$，可以用计算机算出$f(x)$。对于一些对计算机来说是精确叙述的问题如排序，这是可能严格做到的。但对于人脸识别和机器翻译之类的问题，由于涉及到计算机以外的复杂概念，通常不能期望能完全准确地算出。于是，我们退而求其次，找一个能够计算的映射$g$去近似$f$，并且在某种意义下$f$与$g$接近。在过去，人们通过观察去设计这个近似映射$g$，但对于复杂问题人手设计的启发式规则很快会变得难以维护，而且针对个别问题的方法往往过于特殊，导致重复劳动还不利于总结提高。为了用统一的框架解决不同的问题，人们又想办法自动地构造近似映射$g$。当然我们必须给出关于$f$的一些信息才可能完成这构造，通常给出映射在部分点处的值$(x\_1,f(x\_1)),\dots,(x\_M,f(x\_M))$，由此构造$g$的方法叫机器学习。显然即使给定了一个映射在一些点的值，它在其它点处的值仍然可以是任意的，机器学习只能建基于映射能够被相当“简单”的映射逼近的信念。机器学习方法本质上就是假设假定$f$可以被某族映射$\\{g(\cdot;\theta\_1,\dots,\theta\_N)\\}$逼近，然后算出参数$\theta\_1,\dots,\theta\_N$的估计值使$f$与$g$在已知点处的值接近。可见，与基于大数定律的统计方法一样，只有在数据足够多时我们才能指望学习得出的映射$g$确实能近似$f$。幸运的是，随着互联网的兴起，数据源源不断地从人和各种传感器产生，收集数据变得容易，机器学习因而在许多问题变得可行。

由于数学方法一般在欧氏空间中最好处理，因此往往设计一种编码$e: X\to \mathbb{R}^k$把$f$定义域中的对象对应到某个固定维数的欧氏空间，再设计一种解码单射$d: \mathbb{R}^l\to Y$把某个固定维数的欧氏空间中向量对应到$f$值域中的对象，从而可以把问题归结为寻找近似映射$h:\mathbb{R}^k\to\mathbb{R}^l$再令$g=d\circ h\circ e$即可。例如图像可以对应到各像素亮度组成的向量，而文章可以对应到各单词频率组成的向量。

人工神经网络实际上是通过对“简单映射”进行复合来构造逼近函数族的方法，其中的“简单映射”称为神经元。人工神经网络可以用图直观地表示，其中每个顶点是神经元，值沿着有向边在神经元间流动，直至到达输出神经元成为整个逼近映射的值的一个分量。设计人工神经网络时把它分为若干层，大多数边从一层指向下一层。反馈神经网络中也可能存在反向的边，和时序电路中用类似方法实现记忆类似，这种设计被认为可模拟人“越想越像”的记忆，有利于上下文感知。所谓深度学习说是就是层数较多的意思，通常认为后面的层次保存了较高层次（更整体）的信息。

最后指出，虽然上面主要谈监督学习。但某些非监督学习问题可转化为监督学习问题。例如有损压缩问题相当于寻找压缩函数$f:X\to Y$和解压函数$g:Y\to X$使$g\circ f$在某种意义下接近恒同映射，于是我们可以设计一个神经网络，各层中神经元个数先是递减再递增，输入数与输出数相同，数据集中数据同时用作输入和输出去训练网络，最后前半个神经网络就可作为压缩器而后半个神经网络就可作为解压器。类似技术还可以用于生成文本或图像之类。

## 流程

在一个开发迭代中，典型地需要经历以下过程：

1. 理解问题。和其它软件项目一样，我们首先需要理解需求：
    -
2. 收集数据
3. 准备数据
    - 探索数据。通过观察（部分）数据，可以得到一些启发：
        - 通过计算字段值的分布，可能可找出重要的字段。
        - 通过计算字段间的相关性，可能可发现字段间的关系。
        - 通过可视化数据，可能可发现离群值，它们可能对应于诈骗等异常情况。
    - 清理数据。现实世界中收集到的数据质量往往参差不齐，需要经过一些预处理才适合作为机器学习的数据集。
        - 删除重复记录。有时由于各种原因导致同一记录被收集了多次，这时往往应该删去主键相同的重复记录。
        - 删除多余字段。有的字段对建立模型用处不大甚至有误导性，同时也导致建模过程中浪费更多空间和时间，以下字段可能应该删去：
            - 大部分记录值相同的字段。因为能提供的信息较少。
            - 缺失值太多的字段。因为能提供的信息较少，而且往往说明字段的值不易得。
            - 相关性强的变量。因为当两个字段能大致相互决定时，保留一个已经足够。
            - 主键。因为主键只应用于区分记录。
        - 填补缺失值。部分模型不容许数据集有缺失值，这时可以考虑用以下方法补上缺失值：
            - 使用一个普通记录不使用的特殊值（主要适用于枚举型变量）
            - 使用字段忽略缺失的均值、中位数或众数
            - 使用随机值
            - 建立一个用其它字段去预测本字段的模型，然后用预测结果代替缺失值。实际上，所谓的半监督学习就在干这事情。
        - 正规化。部分模型要求不同数值型字段的数量级可比，于是可能需要标准化，例如：
            - 0-1正规化。令某字段的值分别为$x\_i(i=1,\dots,m)$，则分别正规化为$\frac{x\_i-\min\_jx\_j}{\max\_jx\_j-\min\_jx\_j}\in \[0,1\]$。
            - Z正规化。令某字段的值分别为$x\_i(i=1,\dots,m)$，它们的均值为$\mu$、方差为$\sigma$，则分别正规化为$\frac{x\_i-\mu}{\sigma}\in \[0,1\]$，有时也可用协方差同时正规化多个变量。
        - 重新分类。
            - 重新划分。对于枚举型字段，有时字段的值太多或者不同值的频率相差太大，这时一些模型的效果会较差，因而可能需要合并一些类别，甚至可能要为此抛弃部分记录。
            - 数值分箱。对于数值型字段，在希望用离散方法处理时要把数值分为若干类，这可以借助聚类方法，或者粗暴地把分为等长的区间或等频的区间。
        - 审查离群值。数据中可能存在录入错误，以下观察往往有助找出部分可疑记录：
            - 低频值。对于枚举型变量，如果发现一些出现频率特别低的值，则可能有错。
            - 极端值。对于数值型变量，如果发现一些特别大或特别小的值（比如Z正规化后绝对值很大或在特定的分位数范围外），则可能有错。
        - 其它变换。另外还有一些因问题而异的数据变换，比如年龄与出生日期间的转换或者各种格式转换。
    - 降维。有时变量很多但其中有的纯属噪声或者是多余的，不但会浪费空间和时间，还可能起干扰作用（例如多重共线性会使线性回归不稳定），于是可能需要减少变量的个数。
        - 特征选择。即仅保留部分变量而删去其它，原则上我们想选择变量组使模型精度最高或者损失信息与删去变量数相比最小，但由于可能的变量子集太多，通常不会遍历而用启发式策略如：
            - 后向删除。由全部变量出发，每步删掉一个“最无用变量”
            - 前身选择。由没有变量出发，每步加入一个“最有用变量”
        - 特征提取。利用现有的字段去计算另一组字段，比较常见的方法都取现有字段的一些线性组合：
            - 主成分分析。当以协方差矩阵的特征向量组为正交基，则首个分量方差最大（即最“有用”）。通过选择前面若干个主成分（特征向量）使方差（特征值之和）贡献超过某个比例（如50%）作为新的变量组，可以在保持较多信息的情况下减少变量数。另外，应该确保由训练集和测试集求出的主成分近似对应。
            - 因子分析。
    - 划分数据。数据集中的记录通常要分为训练集（用于建立模型）、验证集（用于选择模型，不是所有模型类型都需要）和测试集（用于检验模型）。
        - 比例划分。把数据集随机划分为二到三部分使它们的记录数约为7:3或8:1:1之类的比例。
        - k折交叉验证。把数据集划分为k等份（通常k=10），然后做k个模型，每个模型分别用k-1份数据做训练集而用余下的一份做测试集，最后用这k个测试结果平均出总测试结果。这样可以使测试集更大，并且让每个记录都用于测试一次相对公平。
4. 训练模型
5. 评估模型。因为模型通常对“见过”的数据更为准确（正如一些学生能背出老问题的答案，但题目改一点就不会做了，其实她并没有学会），使用它未“见过”的记录去测试才能检验模型的泛化能力，而不是过度拟合。
6. 部署模型

## 预测

### 分类

#### 最近邻算法

对于每个待分类点，在训练集中找出与之最接近的一个样本点，然后把待分类点判定为这样本点所属的类。最近邻算法非常简单，可以通过调整距离函数来控制不同变量的重要性。最近邻算法的缺点在于分类时需与所有训练样本比对，训练集较大的话时间和空间复杂度比较高。

#### 决策树

决策树是一棵树，每个叶子标上类，而每边标上一个条件。分类时从树根出发，检验各出边对应的条件，沿满足的边走，直到到达叶子即知该判定为哪个类。决策树的优点在于可解释性，缺点在于容易产生过度拟合。构造决策树的有多种方法，但基本方法为递归划分训练集，以下以C4.5为例说明：
1. 把所有训练记录都分配给根
2. 对每个结点
    - 若结点中所有记录属于同一类，则把结点标记为该类
    - 否则，
        1. 找出对结点中记录信息增益最大的变量
        2. 对变量的每个取值建立一个结点并把当前结点对应于该取值的记录都分配给这子结点，并设立当前结点到新结点的边，条件为上述变量有对应取值
3. 重复2.直到树不再变化
4. 利用验证集剪枝：把错误率太高的子树换成叶子，标记为其中样本最常见的类

#### 支持向量机

#### 人工神经网络

#### 元分类器

##### 装包

装包有助

##### 提升


### 回归



## 聚类

## 关联规则 

## 机器学习

## 应用


- 图像技术
    - 文字识别
    - 人脸识别 
    - 图像识别 
    - 图像搜索
- 内容审核
    - 图像审核
    - 文本审核 
    - 视频内容审核
- 自然语言
    - 语言处理基础技术
    - 理解与交互UNIT
- 语音技术
    - 语音识别 
    - 语音合成 
    - 语音唤醒
- 视频技术
    - 视频内容分析
    - 视频封面选图
- 知识图谱
    - 知识图谱Schema 
    - 知识理解
- 增强现实
- 智能客服



    - 多语言分词
    - 文本搜索
    -指令解析
    词性标注
    文本生成
    文本校对
    命名实体
    分类管理
    情感分析
    辅助决策
    商品评价解析
    - 机器翻译
    - 推荐	
                Data cleaning
                Collaborative filtering
                Association rules
                Nearest-neighbor search
                Data stream mining
            Machine learning theory
                Sample complexity and generalization bounds
                Boolean function learning
                Inductive inference
                Query learning
                Regret bounds
- 学习范式
    - 监督学习
        - 排名
        - 学习排名
        - 监督学习分类
        - 监督学习回归
        - 结构化输出
        - 代价感知学习
    - 非监督学习
        - 聚类分析
        - 异常检测
        - 混合模型
        - 主题建模
        - 源分割
        - 主题发现
        - 降维和流形学习
    - 强化学习
        - 顺序决策
        - 反向强化学习
        - 学徒学习
        - 多代理强化学习
        - 敌手学习
    - 多任务学习
        - 转移学习
        - 终生机器学习
        - 协变偏移下学习
- 学习设置
    - 批学习
    - 在线学习
    - 通过演示学习
    - 通过评论学习
    - 通过隐式反馈学习
    - 主动学习
    - 半监督学习
- 机器学习方法：
    - 分类和回归树
    - 核方法
        - 支持向量机
        - 高斯过程
    - 神经网络
    - 逻辑和关系学习
        - 归纳逻辑学习
        - 统计关系学习
    - 概率图模型中学习
        - 最大似然模型
        - 最大熵模型
        - 最大先验模型
        - 混合模型
        - 潜变量模型
        - 贝叶斯网络模型
    - 学习线性模型
        - 感知器算法
    - 分解方法
        - 非负定矩阵分解
        - 因子分析
        - 主成分分析
        - 典型关联分析
        - 潜Dirichlet收集
    - 规则学习
    - 基于实例的学习
    - Markov决策过程
    - 部分可观测的Markov决策过程
    - 随机博弈
    - 学习潜表示
        - 深信念网络
    - 生物启发方法
        - 人工生命
        - 演化硬件
        - 遗传算法
        - 遗传规划
        - 演化机器人
        - 生成和发展方法
- 机器学习算法
    - Markov决策过程的动态规划
        - 迭代
        - Q-学习
        - 策略迭代
        - 时间相关学习
        - 近似动态规划方法
    - 组合方法
        - 提升
        - 装包
    - 特殊方法
    - 特征提取
    - 正则化
- 交叉检验
        
## 结语

当机器真正具备智能时，机器将失去它最大的优点。
